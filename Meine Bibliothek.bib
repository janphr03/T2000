@online{Ki_Fraunhofer,
	title = {Künstliche Intelligenz ({KI}) und maschinelles Lernen - Fraunhofer {IKS}},
	url = {https://www.iks.fraunhofer.de/de/themen/kuenstliche-intelligenz.html},
	urldate = {2025-12-02},
	file = {Künstliche Intelligenz (KI) und maschinelles Lernen - Fraunhofer IKS:files/26/kuenstliche-intelligenz.html:text/html},
}


@book{russell_artificial_1995,
	location = {Upper Saddle River},
	title = {Artificial intelligence: a modern approach},
	isbn = {978-0-13-103805-9 978-0-13-360124-4},
	series = {Prentice Hall series in artificial intelligence},
	shorttitle = {Artificial intelligence},
	pagetotal = {932},
	publisher = {Prentice Hall},
	author = {Russell, Stuart J. and Norvig, Peter},
	date = {1995},
	langid = {english},
	file = {PDF:files/27/Russell und Norvig - 1995 - Artificial intelligence a modern approach.pdf:application/pdf},
}

@misc{ma_comprehensive_2025,
	title = {A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge},
	url = {http://arxiv.org/abs/2310.11703},
	doi = {10.48550/arXiv.2310.11703},
	shorttitle = {A Comprehensive Survey on Vector Database},
	abstract = {Vector databases ({VDBs}) have emerged to manage high-dimensional data that exceed the capabilities of traditional database management systems, and are now tightly integrated with large language models as well as widely applied in modern artificial intelligence systems. Although relatively few studies describe existing or introduce new vector database architectures, the core technologies underlying {VDBs}, such as approximate nearest neighbor search, have been extensively studied and are well documented in the literature. In this work, we present a comprehensive review of the relevant algorithms to provide a general understanding of this booming research area. Specifically, we first provide a review of storage and retrieval techniques in {VDBs}, with detailed design principles and technological evolution. Then, we conduct an in-depth comparison of several advanced {VDB} solutions with their strengths, limitations, and typical application scenarios. Finally, we also outline emerging opportunities for coupling {VDBs} with large language models, including open research problems and trends, such as novel indexing strategies. This survey aims to serve as a practical resource, enabling readers to quickly gain an overall understanding of the current knowledge landscape in this rapidly developing area.},
	number = {{arXiv}:2310.11703},
	publisher = {{arXiv}},
	author = {Ma, Le and Zhang, Ran and Han, Yikun and Yu, Shirui and Wang, Zaitian and Ning, Zhiyuan and Zhang, Jinghan and Xu, Ping and Li, Pengjiang and Ju, Wei and Chen, Chong and Wang, Dongjie and Liu, Kunpeng and Wang, Pengyang and Wang, Pengfei and Fu, Yanjie and Liu, Chunjiang and Zhou, Yuanchun and Lu, Chang-Tien},
	urldate = {2025-12-03},
	date = {2025-06-16},
	eprinttype = {arxiv},
	eprint = {2310.11703 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases},
	file = {Preprint PDF:files/31/Ma et al. - 2025 - A Comprehensive Survey on Vector Database Storage and Retrieval Technique, Challenge.pdf:application/pdf;Snapshot:files/30/2310.html:text/html},
}

@book{mitchell1997machine,
  title     = {Machine Learning},
  author    = {Mitchell, Tom M.},
  year      = {1997},
  publisher = {McGraw-Hill},
  address   = {New York}
}


@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2025-12-07},
	date = {2015-05-28},
	langid = {english},
	file = {PDF:C\:\\Users\\janph\\Zotero\\storage\\DHV5QEQY\\LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}



@article{bengio_representation_2013,
	title = {Representation Learning: A Review and New Perspectives},
	volume = {35},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/6472238},
	doi = {10.1109/TPAMI.2013.50},
	shorttitle = {Representation Learning},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for {AI} is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	pages = {1798--1828},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	urldate = {2025-12-09},
	date = {2013-08},
	keywords = {Abstracts, autoencoder, Boltzmann machine, Deep learning, Feature extraction, feature learning, Learning systems, Machine learning, Manifolds, neural nets, Neural networks, representation learning, Speech recognition, unsupervised learning},
	file = {Eingereichte Version:files/35/Bengio et al. - 2013 - Representation Learning A Review and New Perspectives.pdf:application/pdf;Snapshot:files/36/6472238.html:text/html},
}



@inproceedings{vaswani_attention_2017,
	title = {Attention is All you Need},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 {BLEU} {onEnglish}-to-German translation, improving over the existing best ensemble result by over 1 {BLEU}. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 {BLEU}, achieving a {BLEU} score of 41.1.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	urldate = {2025-12-12},
	date = {2017},
	file = {Full Text PDF:files/40/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}
