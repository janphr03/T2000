\documentclass[12pt,a4paper]{article} % Standard für Hausarbeiten


% Sprachunterst\"utzung f\"ur deutsche Umlaute und Silbentrennung
\usepackage[utf8]{inputenc}  % Zeichencodierung
\usepackage[T1]{fontenc}     % Korrekte Darstellung von Umlauten
\usepackage[ngerman]{babel}  % Deutsche Sprache und Silbentrennung
\usepackage{csquotes}

% Schrift
\usepackage{lmodern}
\usepackage{microtype}

% Seitenr\"ander sch\"oner machen
\usepackage[a4paper, left=3cm, right=3cm, top=2.5cm, bottom=2.5cm]{geometry}

% Mathematik-Symbole
\usepackage{amsmath, amssymb}

% Grafiken und Bilder einf\"ugen
\usepackage{graphicx}  
\usepackage{float}      % Bessere Kontrolle \über die Platzierung

% Tabellen verbessern
\usepackage{array, booktabs}
\usepackage{pifont}     % für Häkchen/Kreuz
\newcommand{\cmark}{\ding{51}} % ✓
\newcommand{\xmark}{\ding{55}} % ✗

% Literaturverzeichnis mit BibTeX
\usepackage[style=ieee, backend=biber]{biblatex}
\addbibresource{literature.bib}
\addbibresource{Meine Bibliothek.bib}

% Quellcode schön darstellen
\usepackage{listings}
\usepackage{xcolor}

\usepackage{tikz}
\usetikzlibrary{positioning, fit, shapes.misc}


\usepackage{needspace} % in der Präambel

\usepackage[acronym]{glossaries}



% --- Grundbegriffe KI / ML ---
\newacronym{ki}{KI}{Künstliche Intelligenz}
\newacronym{ml}{ML}{Maschinelles Lernen}
\newacronym{llm}{LLM}{Large Language Model}
\newacronym{rag}{RAG}{Retrieval-Augmented Generation}
\newacronym{embedding}{Embedding} {Numerische Vektorrepräsentation von Text-, Bild- oder Produktdaten, die semantische Inhalte in einem hochdimensionalen Raum abbildet}

% --- Vektorstore / Vektordatenbank ---
\newacronym{vektorstore}{Vektor-Store}{Leichtgewichtiges System zur Speicherung von Embeddings und Ausführung semantischer Ähnlichkeitssuchen}
\newacronym{vektordatenbank}{Vektordatenbank}{Datenbanksystem für Embeddings mit Mechanismen wie Sharding, Partitionierung, Caching und Replikation}

% --- Speicherverfahren ---
\newacronym{sharding}{Sharding}{Horizontale Datenpartitionierung über mehrere Maschinen zur Skalierung und Lastverteilung}
\newacronym{partitionierung}{Partitionierung}{Aufteilung eines Datensatzes in logisch getrennte Teilmengen}

% --- Caching ---
\newacronym{caching}{Caching}{Zwischenspeicherung häufig genutzter Daten zur Reduktion von Latenz und Systemlast}
\newacronym{fifo}{FIFO}{Cache-Strategie: First-In First-Out}
\newacronym{lru}{LRU}{Cache-Strategie: Least Recently Used}
\newacronym{mru}{MRU}{Cache-Strategie: Most Recently Used}
\newacronym{lfu}{LFU}{Cache-Strategie: Least Frequently Used}

% --- Replikation ---
\newacronym{replikation}{Replikation}{Anlegen mehrerer Datenkopien zur Erhöhung der Verfügbarkeit und Ausfallsicherheit}

% --- Suchverfahren
\newacronym{nns}{NNS}{Exakte Nearest-Neighbor-Suche}
\newacronym{anns}{ANNS}{Approximierende Nearest-Neighbor-Suche}
\newacronym{distanzmaß}{Distanzmaß}{Funktion zur Berechnung der Ähnlichkeit oder Unähnlichkeit zweier Vektoren, z.B. mittels euklidischer Distanz oder Kosinusdistanz}

\makeglossaries



\lstset{ 
    language=Python, % oder C++, Java, Python etc.
    basicstyle=\ttfamily\small, 
    keywordstyle=\color{blue}, 
    commentstyle=\color{gray}, 
    stringstyle=\color{red},
    breaklines=true
}

%Commands/ Style-Definitions====================================

\setlength{\parskip}{1em}    % Abstand zwischen Abs\"atzen
\setlength{\parindent}{0pt}  % Kein Einzug bei neuen Abs\"atzen


% compile \"uber Konsole
% pdflatex test.tex
% biber test
% dann wieder pdflatex test.tex
% ba Datei Pfad anpassen
%T999-Beginn==================================================
\begin{document}


\title{T2000} 
\author{Jan Herrmann}
\date{\today}
\maketitle

%\section{Test}
%\Das ist meine LaTeX-Hausarbeit.
%\parencite{heimeshoff_certification_2024}
%testZitat
%\parencite{kim_planck_2024}


\tableofcontents
\newpage

%–– Im Dokument dann an der Stelle, an der du dein Abkürzungsverzeichnis haben willst ––
\printglossary[type=\acronymtype,title=Abkürzungsverzeichnis]


\newpage
\section{Einleitung}
    \subsection{Motivation}
    \subsection{Problemstellung}
    \subsection{Zielsetzung}




\newpage
\section{Theoretischer Hintergrund}
    
        \subsection{Künstliche Intelligenz}

            Bevor eine präzise Definition von künstlicher Intelligenz möglich ist, muss geklärt werden, welches Ziel ein intelligentes System verfolgen soll. 
            Russell und Norvig zeigen, dass gängige KI-Definitionen in der wissenschaftlichen Literatur entlang zweier zentraler Dimensionen variieren (vgl. \cite{russell_artificial_1995}, Kap.~1.1):

            \begin{itemize}
                \item \textbf{Mensch vs. Rationalität} Soll ein System wie ein Mensch denken oder handeln, oder soll es unabhängig vom Menschen ideal rational agieren?
                \item \textbf{Denken vs. Handeln} Soll Intelligenz anhand interner Denkprozesse oder anhand des beobachtbaren Verhaltens beurteilt werden?
            \end{itemize}

            Aus diesen beiden Dimensionen ergeben sich vier grundlegende Perspektiven auf KI, die unterschiedliche historische Forschungsrichtungen geprägt haben. 
            Eine Übersicht dieser Einordnung zeigt Tabelle~\ref{tab:ki-perspektiven}
        

            \begin{table}[h]
            \centering
            \begin{tabular}{|p{4cm}|p{9cm}|}
            \hline
            \textbf{Kategorie} & \textbf{Beschreibung} \\ \hline
            Systeme, die wie Menschen denken & Fokus auf Nachbildung menschlicher Denkprozesse, z.B. durch kognitive Modelle oder psychologische Theorien. \\ \hline
            Systeme, die wie Menschen handeln & Intelligenz wird anhand menschlich ähnlichen Verhaltens beurteilt, unabhängig vom zugrunde liegenden Denkprozess. \\ \hline
            Systeme, die rational denken & Fokus auf logische Schlussfolgerungen und formale Wissensrepräsentation. \\ \hline
            Systeme, die rational handeln & Intelligente Agenten handeln zielgerichtet und optimal in ihrer Umgebung. \\ \hline
            \end{tabular}
            \caption{Eigene Darstellung in Anlehnung an \cite{russell_artificial_1995}, Kap.~1.1}
            \label{tab:ki-perspektiven}
            \end{table}


            \textbf{Abgrenzung von KI und ML}

            Künstliche Intelligenz (\acrshort{ki}) umfasst alle Verfahren, die darauf abzielen,
            intelligentes Verhalten technisch zu realisieren. Dazu gehören sowohl symbolische
            Ansätze wie Wissensrepräsentation und logisches Schließen als auch
            datengetriebene Methoden zur Wahrnehmung oder Sprachverarbeitung
            (vgl. \cite{russell_artificial_1995}, Kap.~1.1).

            Maschinelles Lernen (\acrshort{ml}) stellt ein klar abgegrenztes Teilgebiet der KI dar.
            Russell und Norvig beschreiben es als das Teilfeld der künstlichen Intelligenz,
            das sich mit Programmen befasst, die aus Erfahrung lernen
            (vgl. \cite{russell_artificial_1995}, Einleitung zu Teil~VI).
            

            Während KI somit als Oberbegriff sämtliche Methoden intelligenter Problemlösung
            einschließt, konzentriert sich ML ausschließlich auf Verfahren, die Wissen nicht
            explizit vorgegeben bekommen, sondern selbstständig aus Daten oder Erfahrungen
            erschließen. ML bildet damit die Grundlage für viele moderne KI-Anwendungen,
            insbesondere für datengetriebene Systeme wie neuronale Netze oder Large Language Models.
            
        % Grafik ML < KI    
        \begin{figure}[htbp]
            \centering
                \begin{tikzpicture}[font=\small]

                    % KI (Obermenge)
                    \filldraw[fill=gray!9, draw=gray!70, thick] (0,0) circle (3cm);
                    \node[font=\normalsize\bfseries] at (-0.4 , 3.2) {Künstliche Intelligenz};

                    % ML (Teilmenge in KI)
                    \filldraw[fill=gray!24, draw=gray!80, thick] (0.9,-0.2) circle (2cm);
                    \node[font=\normalsize\bfseries] at (0.8 , 0.0) {Maschinelles Lernen};

                \end{tikzpicture}
            \caption{Einordnung von Maschinellem Lernen als Teilgebiete der Künstlichen Intelligenz}
            \label{fig:ki-ml-dl}
        \end{figure} 





    

    \subsection{Machine Learning} % 2 Seiten

        %Definition + Einordnung → LeCun et al. 2015
        %Historische Entwicklung / Perzeptron / Backprop / Tiefe Netze → Schmidhuber 2015
        %Architekturbeispiele (Feedforward, CNN, RNN) → eine der beiden oder ein Lehrbuch



        
    \newpage
    \subsection{Vektor-Store}
        
        \acrshort{vektorstore}s bilden die Grundlage für moderne KI-Anwendungen, die auf semantischer Ähnlichkeitssuche basieren. Sie dienen der Speicherung von
        Embeddings, also numerischen Repräsentationen von Text- oder Produktdaten, und ermöglichen effiziente Nearest-Neighbor-Abfragen. 
        Damit stellen sie einen zentralen Baustein für Retrieval-Augmented Generation (RAG) und für Systeme dar, die kontextbezogene Informationen an große Sprachmodelle übergeben (vgl. Abschnitt~\ref{subsec:rag}).

        Da der Begriff „Vektorstore“ in der Literatur häufig als Sammelbezeichnung für leichtgewichtige, embeddingbasierte Speichersysteme verwendet wird,
        ist eine Abgrenzung zu vollwertigen Vektordatenbanken notwendig. 
        Letztere integrieren zusätzliche Mechanismen wie Sharding, interne Partitionierung, Caching oder Replikation 
        und unterscheiden sich damit deutlich in ihrer Komplexität und Skalierbarkeit.

        Im Folgenden werden die in \cite{ma_comprehensive_2025} vorgestellten Speicher- und Suchmechanismen von Vektordatenbanken zusammengefasst 
        und auf das Anwendungsszenario dieser Arbeit übertragen. 
            
        \begin{table}[htbp]
            \centering
            \caption{Abgrenzung von Vektor-Store und Vektordatenbank}
            \label{tab:vektorstore-vektordatenbank}
            \begin{tabular}{p{0.48\textwidth}cc}
                \toprule
                \textbf{Merkmal} & \textbf{Vektor-Store} & \textbf{Vektordatenbank} \\
                \midrule
                Speicherung von Embeddings 
                    & \cmark & \cmark \\
                k-nächste-Nachbarn-Suche (k-NN) 
                    & \cmark & \cmark \\
                Sharding über mehrere Knoten 
                    & \xmark & \cmark \\
                Interne Partitionierung innerhalb eines Knotens 
                    & \xmark & \cmark \\
                Caching-Mechanismen 
                    & \xmark & \cmark \\
                Replikation 
                    & \xmark & \cmark \\
                Approximate Nearest Neighbor (ANNS)-Indexe 
                    & \xmark & \cmark \\
                Metadatenverwaltung und Konsistenzmodelle 
                    & \xmark & \cmark \\
                \bottomrule
            \end{tabular}
        \end{table}




        \subsubsection{Abgrenzung Vektor-Store und Vektor-Datenbank}
            
            Der Begriff \acrshort{vektorstore} wird in der Literatur nicht einheitlich verwendet und dient häufig als 
            Sammelbezeichnung für leichtgewichtige Systeme, die Embeddings speichern und eine grundlegende 
            semantische Ähnlichkeitssuche bereitstellen. Solche Systeme konzentrieren sich in der Regel auf das 
            Einfügen  von Embeddings und die Ausführung von k-nächste-Nachbarn-Abfragen, ohne jedoch 
            erweiterte Datenbankfunktionen bereitzustellen.

            Unter einer \acrshort{vektordatenbank} werden hingegen vollwertige Datenbanksysteme verstanden, die 
            neben der Speicherung und Suche von Embeddings zusätzliche Verwaltungs- und Infrastrukturmechanismen 
            integrieren. Dazu zählen insbesondere Sharding, interne Partitionierung, Caching, Replikation, 
            sowie Indexstrukturen für Approximate-Nearest-Neighbor-Search (\acrshort{anns}). 
            Diese Systeme sind auf hohe Skalierbarkeit, Robustheit und Performanz im produktiven Einsatz ausgelegt.

            Für die vorliegende Arbeit wird daher folgende Unterscheidung getroffen:

            \begin{itemize}
                \item \textbf{Vektor-Store:} leichtgewichtiges System zur Speicherung von Embeddings und zur 
                Durchführung semantischer Ähnlichkeitssuchen.
                \item \textbf{Vektordatenbank:} vollständiges Datenbankmanagementsystem mit skalierbaren 
                Speicher- und Verwaltungsmechanismen.
            \end{itemize}

            Diese definitorische Abgrenzung dient der Klarheit und legt die einheitliche Verwendung der Begriffe im weiteren Verlauf der Arbeit fest.

        \subsubsection{Speicherverfahren für Vektordatenbanken}
            Die im Folgenden beschriebenen Mechanismen stellen zentrale Bestandteile moderner, skalierbarer Vektordatenbanksysteme dar. Sie werden im produktiven Umfeld zur 
            Optimierung von Leistung, Verfügbarkeit und Robustheit eingesetzt. 
            Für den im Rahmen dieser Arbeit entwickelten Prototypen spielen diese Konzepte jedoch keine operative Rolle, 
            da ein leichtgewichtiges Single-Node-System ohne verteilte Architektur eingesetzt wird. 
            Die Ausführungen dienen daher der theoretischen Einordnung und sollen den technologischen Kontext verdeutlichen, in dem sich Vektordatenbanken typischerweise bewegen.


             \textbf{horizontale Datenpartitionierung über mehrere Maschinen} (auch Sharding genannt) bezeichnet ein Verfahren, 
             bei dem eine Datenbank in mehrere logisch getrennte und auf verschiedene physische Knoten verteilte Teilmengen („Shards“) aufgeteilt wird. 
             Durch diese Aufteilung wird das Gesamtdatenset in kleinere, handhabbarere Einheiten zerlegt, 
             was Skalierbarkeit, Lastverteilung und die Verwaltung großer Datenmengen erleichtert (vgl. \cite{ma_comprehensive_2025} S.~3).

            \textbf{horizontale Datenpartitionierung innerhalb einer Maschine} die horizontale Datenpartitionierung innerhalb einer Maschine 
            bezeichnet die Aufteilung der in einer einzelnen Datenbankinstanz gespeicherten Vektordaten in mehrere 
            logisch getrennte Teilmengen („Partitionen“). Im Unterschied zum Sharding, das Daten über mehrere physische Knoten verteilt, 
            erfolgt diese Form der Partitionierung ausschließlich innerhalb eines Systems.
            Ziel ist es, lokale Abfragen effizienter auszuführen, Speicherressourcen besser auszunutzen und parallele Verarbeitung zu ermöglichen.
            Partitionen können beispielsweise über Wertebereiche (Range-Partitioning), vordefinierte Kategorien (List-Partitioning) oder Hash-Verfahren gebildet werden.
            Durch diese interne Strukturierung müssen Suchanfragen nur gegen relevante Partitionen ausgeführt werden, 
            was insbesondere bei großen Einbettungsräumen die Latenz der semantischen Ähnlichkeitssuche reduziert (vgl. \cite{ma_comprehensive_2025}, S.~3f.).

            In modernen Vektordatenbanken wird Partitionierung häufig mit Sharding kombiniert (vgl. Abbildung~\ref{fig:sharding-partitionierung-tikz}).
            
            Während Sharding die Skalierung über mehrere Knoten ermöglicht, 
            optimiert die interne Partitionierung die Datenorganisation innerhalb jedes Shards. 
            Beide Verfahren bilden damit die Grundlage für performante Retrieval-Systeme in Vektor Stores und Vektordatenbanken.

           \begin{figure}[htbp]
                \centering
                \begin{tikzpicture}[
                    shard/.style={
                        rectangle, rounded corners, draw, thick,
                        minimum width=3.5cm, minimum height=2.2cm,
                    },
                    partition/.style={
                        rectangle, draw, thick,
                        minimum width=0.8cm, minimum height=0.8cm
                    },
                    arrow/.style={-stealth, thick}
                ]

                % Anfrage (oben)
                \node[font=\bfseries\normalsize] (anfrage) {Anfrage:};

                % Drei Shards
                \node[shard, below left=1.7cm and 3.5cm of anfrage] (s1) {};
                \node[shard, below=1.7cm of anfrage] (s2) {};
                \node[shard, below right=1.7cm and 3.5cm of anfrage] (s3) {};

                % Labels inside Shards
                \node at (s1) {Shard 1};
                \node at (s2) {Shard 2};
                \node at (s3) {Shard 3};

                % Partitions under Shard 2
                \node[partition, below=0.5cm of s2.south, xshift=-1cm] (p1) {};
                \node[partition, below=0.5cm of s2.south] (p2) {};
                \node[partition, below=0.5cm of s2.south, xshift=1cm] (p3) {};

                % Arrows from Anfrage to Shards
                \draw[arrow] (anfrage.south) -- (s2.north);
                \draw[arrow] ([xshift=-0.2cm] anfrage.south) -- ([yshift=0.2cm] s1.north);
                \draw[arrow] ([xshift=0.2cm] anfrage.south) -- ([yshift=0.2cm] s3.north);

                % Arrows from Shard 2 to Partitions
                \draw[arrow] (s2.south) -- (p1.north);
                \draw[arrow] (s2.south) -- (p2.north);
                \draw[arrow] (s2.south) -- (p3.north);

                % Label Partitions
                \node[below=0.5cm of p2] {\footnotesize Partitionen};

                \end{tikzpicture}

                \caption{Sharding über mehrere Maschinen und interne Partitionierung innerhalb eines Shards}
                \label{fig:sharding-partitionierung-tikz}
            \end{figure}

        
            \textbf{Caching-Mechanismen in Vektordatenbanken} beschreiben das Zwischenspeichern häufig oder kürzlich genutzter Daten in besonders schnellen Speichermedien (z.B. RAM), 
            um Zugriffszeiten zu reduzieren und die Last auf der eigentlichen Datenbank zu verringern. 
            Für Vektordatenbanken ist Caching ein zentraler Mechanismus, da semantische Ähnlichkeitssuchen typischerweise 
            rechenintensiv sind und von wiederholten Abfragen profitieren (vgl. \cite{ma_comprehensive_2025}, S.~4f.).

            Im Gegensatz zu klassischen Schlüssel-Wert-Caches (etwa Redis) ist das Caching in VDBs herausfordernder, 
            da Embeddings hochdimensionale Vektoren darstellen und identische Anfragen selten auftreten. 
            Daher kommen allgemeine Cache-Strategien zum Einsatz, die unabhängig vom genauen Vektorinhalt arbeiten,
            da der Vergleich hochdimensionaler Vektoren zur Bestimmung von Cache-Schlüsseln ungeeignet ist. 
           
            Zu den gängigen Verfahren zählen:
                
            \begin{itemize}
                \item \textbf{First-In First-Out (FIFO):} entfernt das älteste Element im Cache; einfach, aber
                ohne Berücksichtigung der Zugriffshäufigkeit.
                \item \textbf{Least Recently Used (LRU):} löscht den am längsten nicht genutzten Eintrag; gut
                geeignet für Arbeitslasten mit zeitlicher Lokalität.
                \item \textbf{Most Recently Used (MRU):} entfernt das zuletzt genutzte Element; sinnvoll bei
                einmaligen Zugriffsmustern.
                \item \textbf{Least Frequently Used (LFU):} bevorzugt das Entfernen seltener genutzter Einträge;
                vorteilhaft bei stabilen Zugriffshäufigkeiten.
            \end{itemize}

                Einige Systeme nutzen zudem \textbf{partitioniertes Caching}, bei dem Vektordaten in Gruppen (z.B. nach Kategorien oder Zugriffsmustern) getrennt gecacht werden, um Ressourcen gezielt zu optimieren. 
                Insgesamt trägt Caching wesentlich zur Reduktion der Abfragelatenz und zur Stabilisierung der Systemlast bei, insbesondere bei wiederkehrenden Ähnlichkeitsanfragen.


            \textbf{Replikation} bezeichnet das Anlegen und Verteilen mehrerer Kopien von Vektordaten
            auf unterschiedliche Knoten eines verteilten Systems, um Ausfallsicherheit, Verfügbarkeit und Leselastverteilung zu erhöhen.
            Während sie für die semantische Ähnlichkeitssuche nicht unmittelbar leistungsbestimmend ist, stellt sie einen zentralen Mechanismus für die
            betriebliche Robustheit moderner Vektordatenbanken dar (vgl.~\cite{ma_comprehensive_2025}, S.~5f.).

            Insgesamt trägt Replikation wesentlich zur Robustheit und Verfügbarkeit verteilter Vektordatenbanksysteme bei, steht jedoch weniger im direkten
            Zentrum der Ähnlichkeitssuche als vielmehr ihrer infrastrukturellen Betriebsstabilität.

        \subsubsection{Suchverfahren}
            Diese Arbeit befasst sich mit zwei Kategorien von Suchverfahren, deren grundlegende 
            Funktionsweise in Abbildung~\ref{nns-anns-cluster} schematisch gegenübergestellt ist. 
            Die Darstellung der approximierenden Suche stellt hierbei ein vereinfachtes, 
            heuristisches Beispiel dar: Es illustriert das Prinzip, dass ANN-Verfahren den 
            Suchraum gezielt einschränken, ohne jeden Punkt im Vektorraum zu prüfen. 
            In der Praxis existieren verschiedene ANN-Algorithmen (z.B. HNSW, IVF, LSH), die 
            dieses Prinzip auf unterschiedliche Weise umsetzen. Die Abbildung dient somit der 
            didaktischen Veranschaulichung des Grundkonzepts.



            \begin{figure}[htbp]
                \centering
                \begin{tikzpicture}[font=\small]

                    % ========== EXAKTE SUCHE ==========
                    \node[font=\bfseries] at (0, 3) {Exakte Suche (NNS)};

                    % Query Punkt
                    \filldraw[black] (-0.3,0.8) circle (2pt);
                    \node[anchor=east, font=\bfseries] at (-0.35,0.8) {q};

                    % Zufällige Punkte
                    \foreach \i/\x/\y in {
                        a/0.8/0.5,
                        b/1.6/0.7,
                        c/1.2/1.4,
                        d/0.5/1.9,
                        e/1.8/2.3,
                        f/2.3/1.4,
                        g/2.7/0.9,
                        h/2.1/0.4,
                        i/1.4/2.5
                    }{
                        \node[circle, fill=gray!40, inner sep=3pt] (\i) at (\x,\y) {};
                        % direkte Distanzlinien zu q
                        \draw[red!80, dashed] (-0.3,0.8) -- (\i);
                    }

                    % ===== RECHTSEITE: ANN (verbesserte Darstellung) =====
                    \node[font=\bfseries] at (8, 3) {Approximierende Suche (ANNS)};

                    % Query rechts
                    \filldraw[black] (5.8,0.8) circle (2pt);
                    \node[anchor=east, font=\bfseries] at (5.75,0.8) {q};

                    % Cluster Kreise
                    \draw[gray!50, dashed] (6.6,1.0) circle (0.8cm);
                    \draw[gray!50, dashed] (7.9,2.0) circle (0.8cm);
                    \draw[gray!50, dashed] (5.6,2.0) circle (0.8cm);

                    % Punkte im relevanten Cluster (die ANN wirklich prüft)
                    \coordinate (b1) at (6.2,1.2);
                    \coordinate (b2) at (6.7,0.6);
                    \coordinate (b3) at (6.9,1.2);

                    \foreach \p in {b1,b2,b3}{
                        \node[circle, fill=gray!40, inner sep=3pt] at (\p) {};
                    }

                    % Punkte in irrelevanten Clustern (ausgegraut)
                    \foreach \x/\y in {5.3/1.8, 5.8/2.3, 6.1/2.1, 7.7/1.9, 8.0/2.2, 8.2/1.8}{
                        \node[circle, fill=gray!15, inner sep=3pt] at (\x,\y) {};
                    }

                    % ANN: direkte Distanzlinien NUR zu Kandidaten
                    \draw[blue!80, dashed, thick] (5.8,0.8) -- (b2);
                    \draw[blue!80, dashed, thick] (5.8,0.8) -- (b1);
                    \draw[blue!80, dashed, thick] (5.8,0.8) -- (b3);
                \end{tikzpicture}

                
                \caption{
                Vergleich zwischen exakter Nearest-Neighbor-Suche (NNS) und approximierender Suche (ANN).
                Bei exakter Suche berechnet das System die Distanz zwischen der Anfrage und \emph{jedem} Punkt im Raum (rote Linien).
                ANN-Verfahren durchsuchen hingegen nur relevante Regionen des Suchraums (blauer Pfad).
                }
                \label{nns-anns-cluster}
            \end{figure}


            \textbf{Exakte Nearest-Neighbor-Suche (NNS)} bezeichnet das Verfahren, 
            für einen gegebenen Anfragevektor denjenigen Vektor in einer Menge zu bestimmen, 
            der gemäß einem definierten Distanzmaß (z.B. euklidische Distanz) 
            am nächsten liegt. In einfachster Form erfolgt dies durch eine lineare Suche, 
            bei der für jedes gespeicherte Embedding die Distanz zum Anfragevektor berechnet 
            und anschließend der Vektor mit dem geringsten Abstand ausgewählt wird (vgl.~\cite{ma_comprehensive_2025}, S.~6f.).

            Da die Laufzeit dieses Ansatzes linear (O(n)) mit der Anzahl der gespeicherten Embeddings wächst, 
            stoßen Systeme bei großen Datenmengen schnell an ihre Leistungsgrenzen. 
            In solchen Fällen können approximierende Verfahren eingesetzt werden. 
            Die Grundidee besteht darin, eine Lösung zu liefern, die zwar nicht exakt optimal ist, 
            dafür jedoch eine deutlich bessere Laufzeit und geringeren Speicherbedarf ermöglicht. 
            Die leichte Abweichung vom exakten Ergebnis wird somit bewusst zugunsten einer höheren 
            Effizienz in Kauf genommen.

            \textbf{Approximierende Nearest-Neighbor-Suche (ANNS)} verzichtet auf eine 
            vollständige Durchmusterung aller Embeddings und nutzt stattdessen probabilistische oder heuristische Verfahren, 
            um den Suchraum gezielt einzugrenzen. 
            Heuristische Verfahren nutzen vereinfachte Entscheidungsregeln, die eine schnelle, 
            aber nicht zwingend optimale Abschätzung erlauben. 
            Ziel ist es, möglichst schnell einen Vektor zu finden, 
            der dem Anfragevektor sehr ähnlich ist, ohne zwingend den exakt nächsten Nachbarn bestimmen zu müssen. 
            Dadurch lassen sich deutliche Laufzeitgewinne erzielen, insbesondere bei großen Datenmengen (vgl.~\cite{ma_comprehensive_2025}, S.~6f.).

            Während exakte Verfahren insbesondere in kleinen Vektor-Stores mit geringem Datenvolumen eingesetzt werden können, 
            sind approximierende Verfahren bei skalierbaren Vektordatenbanken mit einer großen Anzahl von Embeddings sinnvoll.


    \newpage
             





    \subsection{Large Language Models & Transformer} % 4-5 Seiten
    \subsection{Retrieval-Augmented Generation} % 2-3 Seiten LLM mit Datenquelle
            \label{subsec:rag}

    \subsection{Schnittstellentechnologie} % 2-3 Seiten
    \subsection{prompt Engineering} % 2-3 Seiten

\newpage
\section{Anforderungsanalyse}
    \subsection{Analyse}
    \subsection{Benötigte Daten aus dem PIM System}
    \subsection{Vergleich der LLM Modelle}


\section{Konzeption}
    \subsection{Architektur}
    \subsection{Datenfluss zwischen Vektor Store und Applikation}
    \subsection{Schnittstellendesign}
    \subsection{Promptdesign}


\section{Implementierung}
    \subsection{Überblick über die Systemkomponenten}
    \subsection{Umsetzung der Schnittstellen (Vector Store / Applikation / LLM)}
    \subsection{Datenimport und -export}
    \subsection{Integration des LLMs und Promptlogik}
    \subsection{Fehlerbehandlung und Parallelität}


\section{Evaluation}
        \subsection{Aufbau der Evaluationsbewertung}
        \subsection{Bewertungsmetrik/ -kriterien} %
        \subsection{Durchführung}
        Wer bewertet Texte, Bewertungsbogen, Wie viele Bewertungen pro Text 

        \subsection{Ergebnis}
        
        \subsection{Diskussion}
            LLM ohne RAG halluziniert → schlecht für PIM-Daten (Faktentreue).

            Prompt mit Rollenbeschreibung („Du bist ein Marketing-Experte…“) besser als generischer Prompt.

            Techniktexte brauchen präzisere Struktur -> Template hilft.

            Marketingtexte profitieren von höherer Kreativität → Temperatur-Einstellungen diskutieren.
            
        

\printbibliography  % Literaturverzeichnis einfügen



\end{document}


%Quellen