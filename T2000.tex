\documentclass[12pt,a4paper]{article} % Standard f√ºr Hausarbeiten


% Sprachunterst\"utzung f\"ur deutsche Umlaute und Silbentrennung
\usepackage[utf8]{inputenc}  % Zeichencodierung
\usepackage[T1]{fontenc}     % Korrekte Darstellung von Umlauten
\usepackage[ngerman]{babel}  % Deutsche Sprache und Silbentrennung
\usepackage{csquotes}

% Schrift
\usepackage{lmodern}
\usepackage{microtype}

% Seitenr\"ander sch\"oner machen
\usepackage[a4paper, left=3cm, right=3cm, top=2.5cm, bottom=2.5cm]{geometry}

% Mathematik-Symbole
\usepackage{amsmath, amssymb}

% Grafiken und Bilder einf\"ugen
\usepackage{graphicx}  
\usepackage{float}      % Bessere Kontrolle \√ºber die Platzierung

% Tabellen verbessern
\usepackage{array, booktabs}
\usepackage{pifont}     % f√ºr H√§kchen/Kreuz
\newcommand{\cmark}{\ding{51}} % ‚úì
\newcommand{\xmark}{\ding{55}} % ‚úó

% Literaturverzeichnis mit BibTeX
\usepackage[style=ieee, backend=biber]{biblatex}
\addbibresource{literature.bib}
\addbibresource{Meine Bibliothek.bib}

% Quellcode sch√∂n darstellen
\usepackage{listings}
\usepackage{xcolor}

\usepackage{tikz}
\usetikzlibrary{positioning, fit, shapes.misc}
\usepackage{circuitikz}      
\usetikzlibrary{arrows.meta} % neu dazu


\usepackage{needspace} % in der Pr√§ambel

\usepackage[acronym]{glossaries}



% --- Grundbegriffe KI / ML ---
\newacronym{ki}{KI}{K√ºnstliche Intelligenz}
\newacronym{ml}{ML}{Maschinelles Lernen}
\newacronym{llm}{LLM}{Large Language Model}
\newacronym{rag}{RAG}{Retrieval-Augmented Generation}
\newacronym{embedding}{Embedding} {Numerische Vektorrepr√§sentation von Text-, Bild- oder Produktdaten, die semantische Inhalte in einem hochdimensionalen Raum abbildet}

% --- Machine Learning





% --- Vektorstore / Vektordatenbank ---
\newacronym{vektorstore}{Vektor-Store}{Leichtgewichtiges System zur Speicherung von Embeddings und Ausf√ºhrung semantischer √Ñhnlichkeitssuchen}
\newacronym{vektordatenbank}{Vektordatenbank}{Datenbanksystem f√ºr Embeddings mit Mechanismen wie Sharding, Partitionierung, Caching und Replikation}

% --- Speicherverfahren ---
\newacronym{sharding}{Sharding}{Horizontale Datenpartitionierung √ºber mehrere Maschinen zur Skalierung und Lastverteilung}
\newacronym{partitionierung}{Partitionierung}{Aufteilung eines Datensatzes in logisch getrennte Teilmengen}

% --- Caching ---
\newacronym{caching}{Caching}{Zwischenspeicherung h√§ufig genutzter Daten zur Reduktion von Latenz und Systemlast}
\newacronym{fifo}{FIFO}{Cache-Strategie: First-In First-Out}
\newacronym{lru}{LRU}{Cache-Strategie: Least Recently Used}
\newacronym{mru}{MRU}{Cache-Strategie: Most Recently Used}
\newacronym{lfu}{LFU}{Cache-Strategie: Least Frequently Used}

% --- Replikation ---
\newacronym{replikation}{Replikation}{Anlegen mehrerer Datenkopien zur Erh√∂hung der Verf√ºgbarkeit und Ausfallsicherheit}

% --- Suchverfahren
\newacronym{nns}{NNS}{Exakte Nearest-Neighbor-Suche}
\newacronym{anns}{ANNS}{Approximierende Nearest-Neighbor-Suche}
\newacronym{distanzma√ü}{Distanzma√ü}{Funktion zur Berechnung der √Ñhnlichkeit oder Un√§hnlichkeit zweier Vektoren, z.B. mittels euklidischer Distanz oder Kosinusdistanz}

\makeglossaries



\lstset{ 
    language=Python, % oder C++, Java, Python etc.
    basicstyle=\ttfamily\small, 
    keywordstyle=\color{blue}, 
    commentstyle=\color{gray}, 
    stringstyle=\color{red},
    breaklines=true
}

%Commands/ Style-Definitions====================================

\setlength{\parskip}{1em}    % Abstand zwischen Abs\"atzen
\setlength{\parindent}{0pt}  % Kein Einzug bei neuen Abs\"atzen


% compile \"uber Konsole
% pdflatex test.tex
% biber test
% dann wieder pdflatex test.tex
% ba Datei Pfad anpassen
%T999-Beginn==================================================
\begin{document}


\title{T2000} 
\author{Jan Herrmann}
\date{\today}
\maketitle

%\section{Test}
%\Das ist meine LaTeX-Hausarbeit.
%\parencite{heimeshoff_certification_2024}
%testZitat
%\parencite{kim_planck_2024}


\tableofcontents
\newpage

%‚Äì‚Äì Im Dokument dann an der Stelle, an der du dein Abk√ºrzungsverzeichnis haben willst ‚Äì‚Äì
\printglossary[type=\acronymtype,title=Abk√ºrzungsverzeichnis]


\newpage
\section{Einleitung}
    \subsection{Motivation}
    \subsection{Problemstellung}
    \subsection{Zielsetzung}




\newpage
\section{Theoretischer Hintergrund}
    
    \subsection{K√ºnstliche Intelligenz}

            Bevor eine pr√§zise Definition von k√ºnstlicher Intelligenz m√∂glich ist, muss gekl√§rt werden, welches Ziel ein intelligentes System verfolgen soll. 
            Russell und Norvig zeigen, dass g√§ngige KI-Definitionen in der wissenschaftlichen Literatur entlang zweier zentraler Dimensionen variieren (vgl. \cite{russell_artificial_1995}, Kap.~1.1):

            \begin{itemize}
                \item \textbf{Mensch vs. Rationalit√§t} Soll ein System wie ein Mensch denken oder handeln, oder soll es unabh√§ngig vom Menschen ideal rational agieren?
                \item \textbf{Denken vs. Handeln} Soll Intelligenz anhand interner Denkprozesse oder anhand des beobachtbaren Verhaltens beurteilt werden?
            \end{itemize}

            Aus diesen beiden Dimensionen ergeben sich vier grundlegende Perspektiven auf KI, die unterschiedliche historische Forschungsrichtungen gepr√§gt haben. 
            Eine √úbersicht dieser Einordnung zeigt Tabelle~\ref{tab:ki-perspektiven}
        

            \begin{table}[h]
            \centering
            \begin{tabular}{|p{4cm}|p{9cm}|}
            \hline
            \textbf{Kategorie} & \textbf{Beschreibung} \\ \hline
            Systeme, die wie Menschen denken & Fokus auf Nachbildung menschlicher Denkprozesse, z.B. durch kognitive Modelle oder psychologische Theorien. \\ \hline
            Systeme, die wie Menschen handeln & Intelligenz wird anhand menschlich √§hnlichen Verhaltens beurteilt, unabh√§ngig vom zugrunde liegenden Denkprozess. \\ \hline
            Systeme, die rational denken & Fokus auf logische Schlussfolgerungen und formale Wissensrepr√§sentation. \\ \hline
            Systeme, die rational handeln & Intelligente Agenten handeln zielgerichtet und optimal in ihrer Umgebung. \\ \hline
            \end{tabular}
            \caption{Eigene Darstellung in Anlehnung an \cite{russell_artificial_1995}, Kap.~1.1}
            \label{tab:ki-perspektiven}
            \end{table}


            \textbf{Abgrenzung von KI und ML}

            K√ºnstliche Intelligenz (\acrshort{ki}) umfasst alle Verfahren, die darauf abzielen,
            intelligentes Verhalten technisch zu realisieren. Dazu geh√∂ren sowohl symbolische
            Ans√§tze wie Wissensrepr√§sentation und logisches Schlie√üen als auch
            datengetriebene Methoden zur Wahrnehmung oder Sprachverarbeitung
            (vgl. \cite{russell_artificial_1995}, Kap.~1.1).

            Maschinelles Lernen (\acrshort{ml}) stellt ein klar abgegrenztes Teilgebiet der KI dar.
            Russell und Norvig beschreiben es als das Teilfeld der k√ºnstlichen Intelligenz,
            das sich mit Programmen befasst, die aus Erfahrung lernen
            (vgl. \cite{russell_artificial_1995}, Einleitung zu Teil~VI).
            

            W√§hrend KI somit als Oberbegriff s√§mtliche Methoden intelligenter Probleml√∂sung
            einschlie√üt, konzentriert sich ML ausschlie√ülich auf Verfahren, die Wissen nicht
            explizit vorgegeben bekommen, sondern selbstst√§ndig aus Daten oder Erfahrungen
            erschlie√üen. ML bildet damit die Grundlage f√ºr viele moderne KI-Anwendungen,
            insbesondere f√ºr datengetriebene Systeme wie neuronale Netze oder Large Language Models.
            
            % Grafik ML < KI    
            \begin{figure}[htbp]
                \centering
                    \begin{tikzpicture}[font=\small]

                        % KI (Obermenge)
                        \filldraw[fill=gray!9, draw=gray!70, thick] (0,0) circle (3cm);
                        \node[font=\normalsize\bfseries] at (-0.4 , 3.2) {K√ºnstliche Intelligenz};

                        % ML (Teilmenge in KI)
                        \filldraw[fill=gray!24, draw=gray!80, thick] (0.9,-0.2) circle (2cm);
                        \node[font=\normalsize\bfseries] at (0.8 , 0.0) {Maschinelles Lernen};

                    \end{tikzpicture}
                \caption{Einordnung von Maschinellem Lernen als Teilgebiete der K√ºnstlichen Intelligenz}
                \label{fig:ki-ml-dl}
            \end{figure} 
    %=========================================



    \newpage
    \subsection{Machine Learning} % 2 Seiten

         
        F√ºr die in dieser Arbeit betrachtete Anwendung ist Machine Learning insofern von zentraler Bedeutung, 
        als dass es die Grundlage f√ºr die automatisierte Verarbeitung und semantische Strukturierung von Text- und Produktdaten bildet. 
        Im Folgenden werden zun√§chst konventionelle Verfahren des Machine Learning vorgestellt, bevor anschlie√üend 
        das Representation Learning und das Deep Learning als zunehmend automatisierte und leistungsf√§higere 
        Formen der Merkmalsextraktion betrachtet werden. 
        Abbildung~\ref{fig:ml-rl-dl} veranschaulicht diese Einordnung innerhalb des Gesamtfeldes.


        % Grafik Box DL < RL < ML 
        \begin{figure}[htbp]
            \centering
                \begin{tikzpicture}[
                    box/.style={
                    rectangle,
                    draw=gray!70,
                    thick,
                    rounded corners,
                    minimum width=10cm,
                    minimum height=6cm
                    },
                    innerbox/.style={
                    rectangle,
                    draw=gray!80,
                    thick,
                    rounded corners,
                    minimum width=7cm,
                    minimum height=4cm
                    },
                    corebox/.style={
                    rectangle,
                    draw=gray!90,
                    thick,
                    rounded corners,
                    minimum width=4cm,
                    minimum height=2cm
                    }
                ]

                % Outer box: ML
                \node[box] (ml) {};
                \node at (ml.north) [yshift=-0.6cm, font=\bfseries] {Machine Learning};

                % Middle box: RL
                \node[innerbox] (rl) {};
                \node at (rl.north) [yshift=-0.6cm, font=\bfseries] {Representation Learning};

                % Inner box: DL
                \node[corebox] (dl) {};
                \node at (dl) [font=\bfseries] {Deep Learning};

                \end{tikzpicture}

            \caption{Einordnung von Representation Learning und Deep Learning als Teilgebiete des Machine Learning}
            \label{fig:ml-rl-dl}
        \end{figure}


        \subsubsection{Konventionelles Machine Learning (ML)}
            Nach der klassischen Definition von Mitchell gilt ein Programm als lernf√§hig, wenn \emph{``a computer program is said to learn from experience~E 
            with respect to some class of tasks~T and performance measure~P, if its performance at tasks in~T, 
            as measured by~P, improves with experience~E''} \cite[S.~2]{mitchell1997machine}.
            Konventionelles maschinelles Lernen umfasst somit Verfahren, die statistische 
            Muster in Daten erkennen und auf dieser Basis Vorhersagen oder Entscheidungen treffen k√∂nnen.
        

            LeCun, Bengio und Hinton ordnen maschinelles Lernen als zentralen Bestandteil moderner digitaler Systeme ein. 
            ML-Methoden werden unter anderem zur Objekterkennung in Bildern, zur Umwandlung von Sprache in Text oder 
            zur Relevanzsortierung von Inhalten eingesetzt \cite[S.~436~f.]{lecun_deep_2015}. 
            Diese Anwendungen verdeutlichen den praktischen Nutzen konventioneller ML-Verfahren in einer datengetriebenen Gesellschaft.

            Ein wesentlicher Engpass konventioneller ML-Verfahren besteht darin, dass Modelle Rohdaten wie Pixelwerte, Audiosignale oder Text 
            nicht direkt verarbeiten k√∂nnen.
            Stattdessen m√ºssen die relevanten Merkmale zun√§chst manuell aus den Daten extrahiert werden. 
            Dieser arbeitsintensive Prozess, bei dem Expertinnen und Experten geeignete Eingaberepr√§sentationen gestalten, wird als \emph{Feature 
            Engineering} bezeichnet.
            Da die Modellleistung ma√ügeblich von der Qualit√§t dieser handentwickelten Merkmale abh√§ngt, stellte dieser Ansatz √ºber Jahrzehnte hinweg
            eine zentrale Begrenzung klassischer ML-Systeme dar \cite[S.~436~f.]{lecun_deep_2015}.

            Diese Einschr√§nkung bildet den Ausgangspunkt f√ºr das Representation Learning, das darauf abzielt, 
            Merkmale automatisch aus Rohdaten zu lernen und die Abh√§ngigkeit vom Feature Engineering zu reduzieren.

        
        \subsubsection{Representation Learning (RL)}
            Representation Learning umfasst Methoden, die es erm√∂glichen, aus Rohdaten 
            automatisch diejenigen Merkmale (Features) zu extrahieren, die f√ºr eine bestimmte Aufgabe 
            relevant sind. Im Gegensatz zum konventionellen Machine Learning entf√§llt dabei 
            die Notwendigkeit, Merkmalsextraktoren manuell zu definieren. Stattdessen lernt 
            das Modell eigenst√§ndig geeignete interne Repr√§sentationen, die zentrale 
            Informationen betonen und irrelevante Variationen unterdr√ºcken \cite[S.~436]{lecun_deep_2015}.

            Ein Teil der Leistungsf√§higkeit moderner KI-Systeme beruht darauf, dass diese 
            Repr√§sentationen nicht nur auf einer Ebene entstehen, sondern schrittweise √ºber 
            mehrere Transformationen hinweg verfeinert werden k√∂nnen. Verfahren, die solche 
            hierarchischen Merkmalsr√§ume durch mehrere nichtlineare Schichten lernen, werden 
            unter dem Begriff Deep Learning zusammengefasst.

            % Grafik ML vs RL 
            \begin{figure}[htbp]
                \centering

                \begin{tikzpicture}[
                    font=\small,
                    node distance=1.5cm and 1.4cm,
                    box/.style={
                        rectangle,
                        rounded corners,
                        draw=gray!70,
                        very thick,
                        minimum width=3.4cm,
                        minimum height=1.0cm,
                        align=center,
                        fill=gray!5
                    },
                    arrow/.style={->,>=stealth,thick}
                ]

                % ===== Oben: Konventionelles ML =====
                \node[box] (raw1) {Rohdaten};
                \node[box, right=of raw1, fill=red!15] (feat_manual) {manuell definierte\\Features};
                \node[box, right=of feat_manual] (clf1) {Weiterverarbeiten};

                \draw[arrow] (raw1) -- (feat_manual);
                \draw[arrow] (feat_manual) -- (clf1);

                % √úberschrift oben: mittig √ºber der ganzen Zeile
                \path (raw1) -- (clf1)
                    node[midway, above=0.7cm, font=\footnotesize\bfseries] (titleML)
                        {Konventionelles Machine Learning};

                % ===== Unten: Representation Learning =====
                \node[box, below=1.7cm of raw1] (raw2) {Rohdaten};
                \node[box, right=of raw2, fill=green!15] (feat_learned) {gelernte\\ Repr√§sentationen};
                \node[box, right=of feat_learned] (clf2) {Weiterverarbeiten};

                \draw[arrow] (raw2) -- (feat_learned);
                \draw[arrow] (feat_learned) -- (clf2);

                % √úberschrift unten: ebenfalls mittig √ºber der ganzen Zeile
                \path (raw2) -- (clf2)
                    node[midway, above=0.7cm, font=\footnotesize\bfseries] (titleRL)
                        {Representation Learning};

                \end{tikzpicture}

                \caption{Gegen√ºberstellung von konventionellem Machine Learning und Representation Learning.
                In klassischen Verfahren (rot) m√ºssen relevante Merkmale aus den Rohdaten manuell 
                spezifiziert werden, bevor ein Modell sie weiterverarbeiten kann. 
                Representation Learning (gr√ºn) ersetzt diesen manuellen Schritt durch ein lernf√§higes 
                Abbildungsmodell, das aus den Rohdaten eigenst√§ndig geeignete interne Repr√§sentationen 
                ableitet. Dadurch entstehen schrittweise verfeinerte, hierarchische Merkmalsr√§ume, 
                wie sie im vorangegangenen Text beschrieben wurden und die die Grundlage moderner 
                Deep-Learning-Methoden bilden.}

                \label{fig:representation-learning-schema}
            \end{figure}


        \subsubsection{Deep Learning (DL)}

            Deep Learning stellt eine mehrschichtige Auspr√§gung des Representation Learning dar,
            bei der die Repr√§sentationen der Features nicht nur auf einer Ebene entstehen, sondern
            √ºber zahlreiche hintereinandergeschaltete nichtlineare Transformationen hinweg
            sukzessive abstrahiert und verfeinert werden. 
            Jede dieser Transformationen bildet die Ausgabe einer Schicht in eine neue Repr√§sentation um, 
            die etwas abstrakter ist als die vorherige. 
            Werden gen√ºgend solcher Schichten kombiniert, k√∂nnen tiefe neuronale Netze auch hochkomplexe Funktionen
            modellieren und Strukturen in den Daten erfassen, die mit flachen Modellen nicht zug√§nglich w√§ren (vgl.\cite[S.~436 f.]{lecun_deep_2015}).

            Ein zentraler Vorteil tiefer Architekturen besteht darin, dass in h√∂heren Repr√§sentationsebenen zunehmend 
            abstrakte Merkmale entstehen, die f√ºr die jeweilige Aufgabe besonders relevant sind, 
            w√§hrend aufgabenirrelevante Variationen durch den Lernprozess an Bedeutung verlieren. 
            Diese Selektion erfolgt nicht explizit, sondern ergibt sich implizit aus der Optimierung der Modellparameter 
            hinsichtlich der Zielfunktion. 
            Besonders anschaulich zeigt sich dieses Prinzip in der Bildverarbeitung: 
            Obwohl ein Bild lediglich als Anordnung von Pixelwerten vorliegt, lernen die ersten Schichten 
            eines tiefen Netzes meist einfache Merkmale wie Kanten in unterschiedlichen Orientierungen und Positionen. 
            Die folgenden Schichten erkennen Kombinationen dieser Kanten, etwa Texturen oder kleine Motive. 
            Noch h√∂here Ebenen fassen diese Motive zu komplexeren Strukturen wie Objektteilen zusammen, aus denen 
            schlie√ülich vollst√§ndige Objekte oder abstrakte Konzepte abgeleitet werden k√∂nnen. 
            Der entscheidende Punkt besteht darin, dass keine dieser Merkmalsstufen manuell spezifiziert wird: 
            Die Merkmale und ihre Hierarchie entstehen vollst√§ndig durch den Lernprozess selbst, 
            basierend auf einem allgemeinen Optimierungsverfahren (vgl.\cite[S.~436 f.]{lecun_deep_2015}).

            Diese F√§higkeit, ausgehend von Rohdaten eine gestufte, zunehmend abstrakte Repr√§sentation zu lernen, 
            bildet den Kern des Deep-Learning-Paradigmas und erkl√§rt, warum tiefe neuronale Netze
            in Bereichen wie Computer Vision, Sprachverarbeitung oder allgemeinen Mustererkennungsaufgaben so leistungsf√§hig sind.


        \begin{figure}[htbp]
                \centering
                \begin{tikzpicture}[scale=0.9, font=\small]

                    % --- Oben: flache Abbildung (RL) ---
                    % Achsen
                    \draw[->] (0,0) -- (5,0) node[below] {$x$};
                    \draw[->] (0,0) -- (0,2.2) node[left] {$f(x)$};

                    % einfache, wenig ‚Äûeckige‚Äú Funktion
                    \draw[thick]
                        (0.3,0.4) -- (2.2,1.6) -- (4.7,0.9);

                    \node[anchor=west] at (5.2,0.7) {\footnotesize Representation Learning};

                    % --- Trennlinie ---
                    \draw[gray!50, thick] (-0.2,-0.6) -- (7.3,-0.6);

                    % --- Unten: tiefe Abbildung (Deep Learning) ---
                    \begin{scope}[yshift=-3.2cm]
                        % Achsen
                        \draw[->] (0,0) -- (5,0) node[below] {$x$};
                        \draw[->] (0,0) -- (0,2.2) node[left] {$f(x)$};

                        % stark nichtlineare, ‚Äûum die Ecke gehende‚Äú Funktion
                        \draw[thick]
                            (0.3,0.5) -- (1.1,1.8) -- (2.0,0.6)
                            -- (3.0,1.9) -- (3.8,0.7) -- (4.7,1.6);

                        \node[anchor=west] at (5.2,0.8) {\footnotesize Deep Learning};
                    \end{scope}

                \end{tikzpicture}
                \caption{Schematische Gegen√ºberstellung der durch ein flaches Modell 
                (oben) und ein tiefes neuronales Netz (unten) darstellbaren Funktionen. 
                W√§hrend ein flaches Modell nur relativ einfache, schwach geknickte 
                Abbildungen $f(x)$ realisieren kann, erlauben tiefe Netze durch die 
                Hintereinanderschaltung mehrerer nichtlinearer Schichten stark 
                nichtlineare, ‚Äûum mehrere Ecken gehende‚Äú Funktionen.}
                \label{fig:flach-vs-tief}
        \end{figure}
    %=============================================

    

    \newpage
    \subsection{Transformer und Large Language Modelle} % 4-5 Seiten
        
        wieso weshalb warum
        
        \subsubsection{Transformer-Architektur als Grundlage moderner Sprachmodelle}

         %   Hier vermutlich die Historie rein wieso nicht alles Statisch nacheinander mit Kontextverlust etc.

           % Wortreihenfolge (positioning Encoding) 
           % W√∂rter werden in Nummern zerlegt indem Word-Embedding genutzt werden
           % Ok Video sagt Welle wird gr√∂√üer, je gr√∂√üer die Token Position -> Dadurch hat jedes Wort einen [eigenen Vektor + Positionswert]
           % sin- & cos Werte sind an Position nicht an Wort gebunden dadurch hat das Wort einen Embedding Wert und die Position einen Wert, tauschen sich 
           % die Positionen der Worte √§ndert sich auch das Endergebnis
        %___________________________________________________________________
            




        \subsubsection{Tokenisierung und Eingaberepr√§sentation}
        wie wird Text √ºberhaupt in Embeddings zerlegt
        %__________________________________________________________________

        \subsubsection{Embedding-Layer und Positionsinformation}
        hier kommt positional Encoding ins Spiel
        %__________________________________________________________________

        \subsubsection{Self-Attention als Kontextmechanismus}
            Intuitiv l√§sst sich Self-Attention als Mechanismus verstehen, bei dem jedes Token
            seine Bedeutung nicht isoliert, sondern stets im Kontext der gesamten Sequenz
            bestimmt. Dazu ‚Äûbeobachtet‚Äú ein Token alle anderen Tokens und bewertet, welche
            davon f√ºr seine aktuelle Interpretation relevant sind. 
            Informationen aus besonders wichtigen Tokens werden dabei st√§rker ber√ºcksichtigt,
            w√§hrend weniger relevante Tokens nur einen geringen Einfluss haben.
            Auf diese Weise entsteht f√ºr jedes Token eine kontextabh√§ngige Repr√§sentation,
            die beispielsweise Mehrdeutigkeiten aufl√∂sen oder Referenzen wie Pronomen korrekt
            zuordnen kann.


            % Self Attention Beispiel
            \begin{figure}[htbp]
                \centering
                \begin{tikzpicture}[
                font=\small,
                token/.style={
                    rectangle,
                    rounded corners=3pt,
                    draw=gray!70,
                    fill=gray!6,
                    minimum height=8mm,
                    minimum width=18mm,
                    inner xsep=5mm,
                    align=center
                },
                strong/.style={->, very thick, >=Stealth, blue!70},
                weak/.style={->, thin, >=Stealth, gray!55},
                ]

                % --- Tokens (gleichm√§√üiger Abstand) ---
                \node[token] (der) {Der};
                \node[token, right=10mm of der] (motor) {Motor};
                \node[token, right=10mm of motor] (ist) {ist};
                \node[token, right=10mm of ist] (hitze) {hitzebest√§ndig};

                % --- Hauptbezug: hitzebest√§ndig -> Motor (klar, dominant) ---
                \draw[strong]
                (hitze.north) to[out=140, in=40, looseness=1.2] (motor.north);

                % --- Nebenbez√ºge: hitzebest√§ndig -> ist / der (dezent, kleiner) ---
                \draw[weak]
                (hitze.south) to[out=210, in=330, looseness=1.1] (ist.south);

                \draw[weak]
                (hitze.south) to[out=235, in=315, looseness=1.25] (der.south);

                % --- Optional: kleine Legende (wenn du willst) ---
                \node[anchor=west, text=gray!70] at ([xshift=8mm]hitze.east) {\footnotesize
                \begin{tabular}{@{}l@{}}
                \textcolor{blue!70}{\rule{7mm}{1.2pt}} starker Einfluss\\
                \textcolor{gray!55}{\rule{7mm}{0.6pt}} schwacher Einfluss
                \end{tabular}
                };

                \end{tikzpicture}

                \caption{Intuitive Visualisierung einer Self-Attention-Gewichtung: Das Token \emph{hitzebest√§ndig} 
                bezieht den gr√∂√üten Teil seiner Kontextinformation aus \emph{Motor}. 
                Funktionale Tokens haben geringeren Einfluss.}
                \label{fig:self-attention-deutsch}
            \end{figure}


            Formal wird dieser Mechanismus als gewichtete Summe von Value-Vektoren beschrieben,
            wobei die Gewichte aus einer Kompatibilit√§tsfunktion zwischen Query- und Key-Vektoren
            resultieren (vgl. \cite[S.~3 f.]{vaswani_attention_2017}).


        %__________________________________________________________________
        

        \subsubsection{Large Language Models als skalierte Transformer}

        %__________________________________________________________________



        1. 2.3 LLM vollst√§ndig schreiben

        Du brauchst hier:
        Transformer-Grundlagen
        Tokenisierung
        Embedding Layer
        Aufmerksamkeit

        kurzer Hinweis: LLM erzeugt Embeddings ‚Üí Grundlage f√ºr 2.4
        üëâ Nur ca. 1 Seite.

        Am Ende von 2.3 kurz Embeddings erkl√§ren
        Damit 2.4 nicht unmotiviert Embeddings voraussetzt
    %============================================
    
    

    \newpage
    \subsection{Vektor Store}


        \acrshort{vektorstore}s bilden die Grundlage f√ºr moderne KI-Anwendungen, die auf semantischer √Ñhnlichkeitssuche basieren. Sie dienen der Speicherung von
        Embeddings, also numerischen Repr√§sentationen von Text- oder Produktdaten, und erm√∂glichen effiziente Nearest-Neighbor-Abfragen. 
        Damit stellen sie einen zentralen Baustein f√ºr Retrieval-Augmented Generation (RAG) und f√ºr Systeme dar, die kontextbezogene Informationen an gro√üe Sprachmodelle √ºbergeben (vgl. Abschnitt~\ref{subsec:rag}).

        Da der Begriff ‚ÄûVektor-Store‚Äú in der Literatur h√§ufig als Sammelbezeichnung f√ºr leichtgewichtige, embeddingbasierte Speichersysteme verwendet wird,
        ist eine Abgrenzung zu vollwertigen Vektordatenbanken notwendig. 
        Letztere integrieren zus√§tzliche Mechanismen wie Sharding, interne Partitionierung, Caching oder Replikation 
        und unterscheiden sich damit deutlich in ihrer Komplexit√§t und Skalierbarkeit.

        Im Folgenden werden die in \cite{ma_comprehensive_2025} vorgestellten Speicher- und Suchmechanismen von Vektordatenbanken zusammengefasst 
        und auf das Anwendungsszenario dieser Arbeit √ºbertragen. 
                
        \begin{table}[htbp]
            \centering
            \caption{Abgrenzung von Vektor-Store und Vektordatenbank}
            \label{tab:vektorstore-vektordatenbank}
            \begin{tabular}{p{0.48\textwidth}cc}
                \toprule
                \textbf{Merkmal} & \textbf{Vektor-Store} & \textbf{Vektordatenbank} \\
                \midrule
                Speicherung von Embeddings 
                    & \cmark & \cmark \\
                k-n√§chste-Nachbarn-Suche (k-NN) 
                    & \cmark & \cmark \\
                Sharding √ºber mehrere Knoten 
                    & \xmark & \cmark \\
                Interne Partitionierung innerhalb eines Knotens 
                    & \xmark & \cmark \\
                Caching-Mechanismen 
                    & \xmark & \cmark \\
                Replikation 
                    & \xmark & \cmark \\
                Approximate Nearest Neighbor (ANNS)-Indexe 
                    & \xmark & \cmark \\
                Metadatenverwaltung und Konsistenzmodelle 
                    & \xmark & \cmark \\
                \bottomrule
            \end{tabular}
        \end{table}



        \subsubsection{Abgrenzung Vektor-Store und Vektor-Datenbank}
        
        Der Begriff \acrshort{vektorstore} wird in der Literatur nicht einheitlich verwendet und dient h√§ufig als 
        Sammelbezeichnung f√ºr leichtgewichtige Systeme, die Embeddings speichern und eine grundlegende 
        semantische √Ñhnlichkeitssuche bereitstellen. Solche Systeme konzentrieren sich in der Regel auf das 
        Einf√ºgen  von Embeddings und die Ausf√ºhrung von k-n√§chste-Nachbarn-Abfragen, ohne jedoch 
        erweiterte Datenbankfunktionen bereitzustellen.

        Unter einer \acrshort{vektordatenbank} werden hingegen vollwertige Datenbanksysteme verstanden, die 
        neben der Speicherung und Suche von Embeddings zus√§tzliche Verwaltungs- und Infrastrukturmechanismen 
        integrieren. Dazu z√§hlen insbesondere Sharding, interne Partitionierung, Caching, Replikation, 
        sowie Indexstrukturen f√ºr Approximate-Nearest-Neighbor-Search (\acrshort{anns}). 
        Diese Systeme sind auf hohe Skalierbarkeit, Robustheit und Performanz im produktiven Einsatz ausgelegt.
        
        F√ºr die vorliegende Arbeit wird daher folgende Unterscheidung getroffen:

        \begin{itemize}
            \item \textbf{Vektor-Store:} leichtgewichtiges System zur Speicherung von Embeddings und zur 
            Durchf√ºhrung semantischer √Ñhnlichkeitssuchen.
            \item \textbf{Vektordatenbank:} vollst√§ndiges Datenbankmanagementsystem mit skalierbaren 
            Speicher- und Verwaltungsmechanismen.
        \end{itemize}

        Diese definitorische Abgrenzung dient der Klarheit und legt die einheitliche Verwendung der Begriffe im weiteren Verlauf der Arbeit fest.


  
        Die im Folgenden beschriebenen Mechanismen stellen zentrale Bestandteile moderner, skalierbarer Vektordatenbanksysteme dar. Sie werden im produktiven Umfeld zur 
        Optimierung von Leistung, Verf√ºgbarkeit und Robustheit eingesetzt. 
        F√ºr den im Rahmen dieser Arbeit entwickelten Prototypen spielen diese Konzepte jedoch keine operative Rolle, 
        da ein einnfacher Vektor-Store eingesetzt wird. 
        Die Ausf√ºhrungen dienen ausschlie√ülich der theoretischen Einordnung und der Abgrenzung des in dieser Arbeit 
        eingesetzten Vektor-Stores gegen√ºber produktiven Vektordatenbanksystemen.
        
            \subsubsection{Speicherverfahren f√ºr Vektordatenbanken}


            \textbf{horizontale Datenpartitionierung √ºber mehrere Maschinen} (auch Sharding genannt) bezeichnet ein Verfahren, 
            bei dem eine Datenbank in mehrere logisch getrennte und auf verschiedene physische Knoten verteilte Teilmengen (‚ÄûShards‚Äú) aufgeteilt wird. 
            Durch diese Aufteilung wird das Gesamtdatenset in kleinere, handhabbarere Einheiten zerlegt, 
            was Skalierbarkeit, Lastverteilung und die Verwaltung gro√üer Datenmengen erleichtert (vgl. \cite[S.~3f]{ma_comprehensive_2025}).

            Die \textbf{horizontale Datenpartitionierung innerhalb einer Maschine} 
            bezeichnet die Aufteilung der in einer einzelnen Datenbankinstanz gespeicherten Vektordaten in mehrere 
            logisch getrennte Teilmengen (‚ÄûPartitionen‚Äú). Im Unterschied zum Sharding, das Daten √ºber mehrere physische Knoten verteilt, 
            erfolgt diese Form der Partitionierung ausschlie√ülich innerhalb eines Systems.
            Ziel ist es, lokale Abfragen effizienter auszuf√ºhren, Speicherressourcen besser auszunutzen und parallele Verarbeitung zu erm√∂glichen.
            Partitionen k√∂nnen beispielsweise √ºber Wertebereiche (Range-Partitioning), vordefinierte Kategorien (List-Partitioning) oder Hash-Verfahren gebildet werden.
            Durch diese interne Strukturierung m√ºssen Suchanfragen nur gegen relevante Partitionen ausgef√ºhrt werden, 
            was insbesondere bei gro√üen Einbettungsr√§umen die Latenz der semantischen √Ñhnlichkeitssuche reduziert (vgl. \cite[S.~3f.]{ma_comprehensive_2025}).

            In modernen Vektordatenbanken wird Partitionierung h√§ufig mit Sharding kombiniert (vgl. Abbildung~\ref{fig:sharding-partitionierung-tikz}).
            
            W√§hrend Sharding die Skalierung √ºber mehrere Knoten erm√∂glicht, 
            optimiert die interne Partitionierung die Datenorganisation innerhalb jedes Shards. 
            Beide Verfahren bilden damit die Grundlage f√ºr performante Retrieval-Systeme in Vektor Stores und Vektordatenbanken.

            \begin{figure}[htbp]
                \centering
                \begin{tikzpicture}[
                    shard/.style={
                        rectangle, rounded corners, draw, thick,
                        minimum width=3.5cm, minimum height=2.2cm,
                    },
                    partition/.style={
                        rectangle, draw, thick,
                        minimum width=0.8cm, minimum height=0.8cm
                    },
                    arrow/.style={-stealth, thick}
                ]

                % Anfrage (oben)
                \node[font=\bfseries\normalsize] (anfrage) {Anfrage:};

                % Drei Shards
                \node[shard, below left=1.7cm and 3.5cm of anfrage] (s1) {};
                \node[shard, below=1.7cm of anfrage] (s2) {};
                \node[shard, below right=1.7cm and 3.5cm of anfrage] (s3) {};

                % Labels inside Shards
                \node at (s1) {Shard 1};
                \node at (s2) {Shard 2};
                \node at (s3) {Shard 3};

                % Partitions under Shard 2
                \node[partition, below=0.5cm of s2.south, xshift=-1cm] (p1) {};
                \node[partition, below=0.5cm of s2.south] (p2) {};
                \node[partition, below=0.5cm of s2.south, xshift=1cm] (p3) {};

                % Arrows from Anfrage to Shards
                \draw[arrow] (anfrage.south) -- (s2.north);
                \draw[arrow] ([xshift=-0.2cm] anfrage.south) -- ([yshift=0.2cm] s1.north);
                \draw[arrow] ([xshift=0.2cm] anfrage.south) -- ([yshift=0.2cm] s3.north);

                % Arrows from Shard 2 to Partitions
                \draw[arrow] (s2.south) -- (p1.north);
                \draw[arrow] (s2.south) -- (p2.north);
                \draw[arrow] (s2.south) -- (p3.north);

                % Label Partitions
                \node[below=0.5cm of p2] {\footnotesize Partitionen};

                \end{tikzpicture}

                \caption{Sharding √ºber mehrere Maschinen und interne Partitionierung innerhalb eines Shards}
                \label{fig:sharding-partitionierung-tikz}
            \end{figure}

        
            \textbf{Caching-Mechanismen in Vektordatenbanken} beschreiben das Zwischenspeichern h√§ufig oder k√ºrzlich genutzter Daten in besonders schnellen Speichermedien (z.B. RAM), 
            um Zugriffszeiten zu reduzieren und die Last auf der eigentlichen Datenbank zu verringern. 
            F√ºr Vektordatenbanken ist Caching ein zentraler Mechanismus, da semantische √Ñhnlichkeitssuchen typischerweise 
            rechenintensiv sind und von wiederholten Abfragen profitieren (vgl. \cite[S.~4f.]{ma_comprehensive_2025}).

            Im Gegensatz zu klassischen Schl√ºssel-Wert-Caches (etwa Redis) ist das Caching in VDBs herausfordernder, 
            da Embeddings hochdimensionale Vektoren darstellen und identische Anfragen selten auftreten. 
            Daher kommen allgemeine Cache-Strategien zum Einsatz, die unabh√§ngig vom genauen Vektorinhalt arbeiten,
            da der Vergleich hochdimensionaler Vektoren zur Bestimmung von Cache-Schl√ºsseln ungeeignet ist. 
            
            Zu den g√§ngigen Verfahren z√§hlen:
            
            \begin{itemize}
                \item \textbf{First-In First-Out (FIFO):} entfernt das √§lteste Element im Cache; einfach, aber
                ohne Ber√ºcksichtigung der Zugriffsh√§ufigkeit.
                \item \textbf{Least Recently Used (LRU):} l√∂scht den am l√§ngsten nicht genutzten Eintrag; gut
                geeignet f√ºr Arbeitslasten mit zeitlicher Lokalit√§t.
                \item \textbf{Most Recently Used (MRU):} entfernt das zuletzt genutzte Element; sinnvoll bei
                einmaligen Zugriffsmustern.
                \item \textbf{Least Frequently Used (LFU):} bevorzugt das Entfernen seltener genutzter Eintr√§ge;
                vorteilhaft bei stabilen Zugriffsh√§ufigkeiten.
            \end{itemize}

            Einige Systeme nutzen zudem \textbf{partitioniertes Caching}, bei dem Vektordaten in Gruppen (z.B. nach Kategorien oder Zugriffsmustern) getrennt gecacht werden, um Ressourcen gezielt zu optimieren. 
            Insgesamt tr√§gt Caching wesentlich zur Reduktion der Abfragelatenz und zur Stabilisierung der Systemlast bei, insbesondere bei wiederkehrenden √Ñhnlichkeitsanfragen.


            \textbf{Replikation} bezeichnet das Anlegen und Verteilen mehrerer Kopien von Vektordaten
            auf unterschiedliche Knoten eines verteilten Systems, um Ausfallsicherheit, Verf√ºgbarkeit und Leselastverteilung zu erh√∂hen.
            W√§hrend sie f√ºr die semantische √Ñhnlichkeitssuche nicht unmittelbar leistungsbestimmend ist, stellt sie einen zentralen Mechanismus f√ºr die
            betriebliche Robustheit moderner Vektordatenbanken dar (vgl.~\cite[S.~5f.]{ma_comprehensive_2025}).

            Insgesamt tr√§gt Replikation wesentlich zur Robustheit und Verf√ºgbarkeit verteilter Vektordatenbanksysteme bei, steht jedoch weniger im direkten
            Zentrum der √Ñhnlichkeitssuche als vielmehr ihrer infrastrukturellen Betriebsstabilit√§t.

        
        \subsubsection{Suchverfahren}
            Diese Arbeit befasst sich mit zwei Kategorien von Suchverfahren, deren grundlegende 
            Funktionsweise in Abbildung~\ref{nns-anns-cluster} schematisch gegen√ºbergestellt ist. 
            Die Darstellung der approximierenden Suche stellt hierbei ein vereinfachtes, 
            heuristisches Beispiel dar: Es illustriert das Prinzip, dass ANN-Verfahren den 
            Suchraum gezielt einschr√§nken, ohne jeden Punkt im Vektorraum zu pr√ºfen. 
            In der Praxis existieren verschiedene ANN-Algorithmen (z.B. HNSW, IVF, LSH), die 
            dieses Prinzip auf unterschiedliche Weise umsetzen. Die Abbildung dient somit der 
            didaktischen Veranschaulichung des Grundkonzepts.


        % Grafik ANNS vs NNS
        \begin{figure}[htbp]
            \centering
            \begin{tikzpicture}[font=\small]

                % ========== EXAKTE SUCHE ==========
                \node[font=\bfseries] at (0, 3) {Exakte Suche (NNS)};

                % Query Punkt
                \filldraw[black] (-0.3,0.8) circle (2pt);
                \node[anchor=east, font=\bfseries] at (-0.35,0.8) {q};

                % Zuf√§llige Punkte
                \foreach \i/\x/\y in {
                    a/0.8/0.5,
                    b/1.6/0.7,
                    c/1.2/1.4,
                    d/0.5/1.9,
                    e/1.8/2.3,
                    f/2.3/1.4,
                    g/2.7/0.9,
                    h/2.1/0.4,
                    i/1.4/2.5
                }{
                    \node[circle, fill=gray!40, inner sep=3pt] (\i) at (\x,\y) {};
                    % direkte Distanzlinien zu q
                    \draw[red!80, dashed] (-0.3,0.8) -- (\i);
                }

                % ===== RECHTSEITE: ANN (verbesserte Darstellung) =====
                \node[font=\bfseries] at (8, 3) {Approximierende Suche (ANNS)};

                % Query rechts
                \filldraw[black] (5.8,0.8) circle (2pt);
                \node[anchor=east, font=\bfseries] at (5.75,0.8) {q};

                % Cluster Kreise
                \draw[gray!50, dashed] (6.6,1.0) circle (0.8cm);
                \draw[gray!50, dashed] (7.9,2.0) circle (0.8cm);
                \draw[gray!50, dashed] (5.6,2.0) circle (0.8cm);

                % Punkte im relevanten Cluster (die ANN wirklich pr√ºft)
                \coordinate (b1) at (6.2,1.2);
                \coordinate (b2) at (6.7,0.6);
                \coordinate (b3) at (6.9,1.2);

                \foreach \p in {b1,b2,b3}{
                    \node[circle, fill=gray!40, inner sep=3pt] at (\p) {};
                }

                % Punkte in irrelevanten Clustern (ausgegraut)
                \foreach \x/\y in {5.3/1.8, 5.8/2.3, 6.1/2.1, 7.7/1.9, 8.0/2.2, 8.2/1.8}{
                    \node[circle, fill=gray!15, inner sep=3pt] at (\x,\y) {};
                }

                % ANN: direkte Distanzlinien NUR zu Kandidaten
                \draw[blue!80, dashed, thick] (5.8,0.8) -- (b2);
                \draw[blue!80, dashed, thick] (5.8,0.8) -- (b1);
                \draw[blue!80, dashed, thick] (5.8,0.8) -- (b3);
            \end{tikzpicture}

            
            \caption{
            Vergleich zwischen exakter Nearest-Neighbor-Suche (NNS) und approximierender Suche (ANN).
            Bei exakter Suche berechnet das System die Distanz zwischen der Anfrage und \emph{jedem} Punkt im Raum (rote Linien).
            ANN-Verfahren durchsuchen hingegen nur relevante Regionen des Suchraums (blauer Pfad).
            }
            \label{nns-anns-cluster}
        \end{figure}


        \textbf{Exakte Nearest-Neighbor-Suche (NNS)} bezeichnet das Verfahren, 
        f√ºr einen gegebenen Anfragevektor denjenigen Vektor in einer Menge zu bestimmen, 
        der gem√§√ü einem definierten Distanzma√ü (z.B. euklidische Distanz) 
        am n√§chsten liegt. In einfachster Form erfolgt dies durch eine lineare Suche, 
        bei der f√ºr jedes gespeicherte Embedding die Distanz zum Anfragevektor berechnet 
        und anschlie√üend der Vektor mit dem geringsten Abstand ausgew√§hlt wird (vgl.~\cite[S.~6f.]{ma_comprehensive_2025}).

        Da die Laufzeit dieses Ansatzes linear (O(n)) mit der Anzahl der gespeicherten Embeddings w√§chst, 
        sto√üen Systeme bei gro√üen Datenmengen schnell an ihre Leistungsgrenzen. 
        In solchen F√§llen k√∂nnen approximierende Verfahren eingesetzt werden. 
        Die Grundidee besteht darin, eine L√∂sung zu liefern, die zwar nicht exakt optimal ist, 
        daf√ºr jedoch eine deutlich bessere Laufzeit und geringeren Speicherbedarf erm√∂glicht. 
        Die leichte Abweichung vom exakten Ergebnis wird somit bewusst zugunsten einer h√∂heren 
        Effizienz in Kauf genommen.

        \textbf{Approximierende Nearest-Neighbor-Suche (ANNS)} verzichtet auf eine 
        vollst√§ndige Durchmusterung aller Embeddings und nutzt stattdessen probabilistische oder heuristische Verfahren, 
        um den Suchraum gezielt einzugrenzen. 
        Heuristische Verfahren nutzen vereinfachte Entscheidungsregeln, die eine schnelle, 
        aber nicht zwingend optimale Absch√§tzung erlauben. 
        Ziel ist es, m√∂glichst schnell einen Vektor zu finden, 
        der dem Anfragevektor sehr √§hnlich ist, ohne zwingend den exakt n√§chsten Nachbarn bestimmen zu m√ºssen. 
        Dadurch lassen sich deutliche Laufzeitgewinne erzielen, insbesondere bei gro√üen Datenmengen (vgl.~\cite[S.~6f.]{ma_comprehensive_2025}).

        W√§hrend exakte Verfahren insbesondere in kleinen Vektor-Stores mit geringem Datenvolumen eingesetzt werden k√∂nnen, 
        sind approximierende Verfahren bei skalierbaren Vektordatenbanken mit einer gro√üen Anzahl von Embeddings sinnvoll.


    
    
    %============================================
    


        \newpage
    \subsection{Schnittstellentechnologie} % 2-3 Seiten
    %==============================================
    


       \newpage
    \subsection{Retrieval-Augmented Generation} % 2-3 Seiten LLM mit Datenquelle
    %============================================




    
    
    
    
    \newpage
    \subsection{prompt Engineering} % 2-3 Seiten
    %=============================================



\newpage
\section{Anforderungsanalyse}
    \subsection{Analyse}
    \subsection{Ben√∂tigte Daten aus dem PIM System}
    \subsection{Vergleich der LLM Modelle}


\section{Konzeption}
    \subsection{Architektur}
    \subsection{Datenfluss zwischen Vektor Store und Applikation}
    \subsection{Schnittstellendesign}
    \subsection{Promptdesign}


\section{Implementierung}
    \subsection{√úberblick √ºber die Systemkomponenten}
    \subsection{Umsetzung der Schnittstellen (Vector Store / Applikation / LLM)}
    \subsection{Datenimport und -export}
    \subsection{Integration des LLMs und Promptlogik}
    \subsection{Fehlerbehandlung und Parallelit√§t}


\section{Evaluation}
        \subsection{Aufbau der Evaluationsbewertung}
        \subsection{Bewertungsmetrik/ -kriterien} %
        \subsection{Durchf√ºhrung}
        Wer bewertet Texte, Bewertungsbogen, Wie viele Bewertungen pro Text 

        \subsection{Ergebnis}
        
        \subsection{Diskussion}
            LLM ohne RAG halluziniert ‚Üí schlecht f√ºr PIM-Daten (Faktentreue).

            Prompt mit Rollenbeschreibung (‚ÄûDu bist ein Marketing-Experte‚Ä¶‚Äú) besser als generischer Prompt.

            Techniktexte brauchen pr√§zisere Struktur -> Template hilft.

            Marketingtexte profitieren von h√∂herer Kreativit√§t ‚Üí Temperatur-Einstellungen diskutieren.
            
        

\printbibliography  % Literaturverzeichnis einf√ºgen



\end{document}


%Quellen