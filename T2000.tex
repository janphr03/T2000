\documentclass[12pt,a4paper]{article} % Standard für Hausarbeiten

% Sprachunterst\"utzung f\"ur deutsche Umlaute und Silbentrennung
\usepackage[utf8]{inputenc}  % Zeichencodierung
\usepackage[T1]{fontenc}     % Korrekte Darstellung von Umlauten
\usepackage[ngerman]{babel}  % Deutsche Sprache und Silbentrennung
\usepackage{csquotes}

% Seitenr\"ander sch\"oner machen
\usepackage[a4paper, left=3cm, right=3cm, top=2.5cm, bottom=2.5cm]{geometry}

% Mathematik-Symbole
\usepackage{amsmath, amssymb}

% Grafiken und Bilder einf\"ugen
\usepackage{graphicx}  
\usepackage{float}      % Bessere Kontrolle \über die Platzierung

% Tabellen verbessern
\usepackage{array, booktabs}

% Literaturverzeichnis mit BibTeX
\usepackage[style=ieee, backend=biber]{biblatex}
\addbibresource{literature.bib}
\addbibresource{Meine Bibliothek.bib}

% Quellcode schön darstellen
\usepackage{listings}
\usepackage{xcolor}

\usepackage{needspace} % in der Präambel

\usepackage[acronym]{glossaries}



\newacronym{ki}{KI}{künstliche Intelligenz}
\newacronym{ml}{ML}{Machine learning}


\makeglossaries



\lstset{ 
    language=C, % oder C++, Java, Python etc.
    basicstyle=\ttfamily\small, 
    keywordstyle=\color{blue}, 
    commentstyle=\color{gray}, 
    stringstyle=\color{red},
    breaklines=true
}

%Commands/ Style-Definitions====================================
\newcommand{\csharp}{C\#}

\setlength{\parskip}{1em}    % Abstand zwischen Abs\"atzen
\setlength{\parindent}{0pt}  % Kein Einzug bei neuen Abs\"atzen


% compile \"uber Konsole
% pdflatex test.tex
% biber test
% dann wieder pdflatex test.tex
% ba Datei Pfad anpassen
%T999-Beginn==================================================
\begin{document}


\title{T2000} 
\author{Jan Herrmann}
\date{\today}
\maketitle

%\section{Test}
%\Das ist meine LaTeX-Hausarbeit.
%\parencite{heimeshoff_certification_2024}
%testZitat
%\parencite{kim_planck_2024}


\tableofcontents
\newpage

%–– Im Dokument dann an der Stelle, an der du dein Abkürzungsverzeichnis haben willst ––
\printglossary[type=\acronymtype,title=Abkürzungsverzeichnis]



\section{Einleitung}
    \subsection{Motivation}
    \subsection{Problemstellung}
    \subsection{Zielsetzung}

DEVELOP Test
\acrshort{ML}


\section{Theoretischer Hintergrund}
    


        \subsection{Künstliche Intelligenz}

            Bevor eine präzise Definition von künstlicher Intelligenz möglich ist, muss geklärt werden, welches Ziel ein intelligentes System verfolgen soll. 
            Russell und Norvig zeigen, dass gängige KI-Definitionen in der wissenschaftlichen Literatur entlang zweier zentraler Dimensionen variieren (vgl. \cite{russell_artificial_1995}, Kap.~1.1):

            \begin{itemize}
                \item \textbf{Mensch vs. Rationalität} Soll ein System wie ein Mensch denken oder handeln, oder soll es unabhängig vom Menschen ideal rational agieren?
                \item \textbf{Denken vs. Handeln} Soll Intelligenz anhand interner Denkprozesse oder anhand des beobachtbaren Verhaltens beurteilt werden?
            \end{itemize}

            Aus diesen beiden Dimensionen ergeben sich vier grundlegende Perspektiven auf KI, die unterschiedliche historische Forschungsrichtungen geprägt haben. 
            Eine Übersicht dieser Einordnung zeigt Tabelle~\ref{tab:ki-perspektiven}
        

            \begin{table}[h]
            \centering
            \begin{tabular}{|p{4cm}|p{9cm}|}
            \hline
            \textbf{Kategorie} & \textbf{Beschreibung} \\ \hline
            Systeme, die wie Menschen denken & Fokus auf Nachbildung menschlicher Denkprozesse, z.B. durch kognitive Modelle oder psychologische Theorien. \\ \hline
            Systeme, die wie Menschen handeln & Intelligenz wird anhand menschlich ähnlichen Verhaltens beurteilt, unabhängig vom zugrunde liegenden Denkprozess. \\ \hline
            Systeme, die rational denken & Fokus auf logische Schlussfolgerungen und formale Wissensrepräsentation. \\ \hline
            Systeme, die rational handeln & Intelligente Agenten handeln zielgerichtet und optimal in ihrer Umgebung. \\ \hline
            \end{tabular}
            \caption{Eigene Darstellung in Anlehnung an \cite{russell_artificial_1995}, Kap.~1.1}
            \label{tab:ki-perspektiven}
            \end{table}


            \textbf{Abgrenzung von KI und ML}

            Künstliche Intelligenz (\acrshort{ki}) umfasst alle Verfahren, die darauf abzielen,
            intelligentes Verhalten technisch zu realisieren. Dazu gehören sowohl symbolische
            Ansätze wie Wissensrepräsentation und logisches Schließen als auch
            datengetriebene Methoden zur Wahrnehmung oder Sprachverarbeitung
            (vgl. \cite{russell_artificial_1995}, Kap.~1.1).

            Maschinelles Lernen (\acrshort{ml}) stellt ein klar abgegrenztes Teilgebiet der KI dar.
            Russell und Norvig beschreiben es als das Teilfeld der künstlichen Intelligenz,
            das sich mit Programmen befasst, die aus Erfahrung lernen
            (vgl. \cite{russell_artificial_1995}, Einleitung zu Teil~VI).
            

            Während KI somit als Oberbegriff sämtliche Methoden intelligenter Problemlösung
            einschließt, konzentriert sich ML ausschließlich auf Verfahren, die Wissen nicht
            explizit vorgegeben bekommen, sondern selbstständig aus Daten oder Erfahrungen
            erschließen. ML bildet damit die Grundlage für viele moderne KI-Anwendungen,
            insbesondere für datengetriebene Systeme wie neuronale Netze oder Large Language Models.
            


        

    \subsection{Vektorstore}
        
        Vektor Stores bilden die Grundlage für moderne KI-Anwendungen, die auf semantischer Ähnlichkeitssuche basieren. Sie dienen der Speicherung von
        Embeddings, also numerischen Repräsentationen von Text- oder Produktdaten, und ermöglichen effiziente Nearest-Neighbor-Abfragen. 
        Damit stellen sie einen zentralen Baustein für Retrieval-Augmented Generation (RAG) und für Systeme dar, die kontextbezogene Informationen an große Sprachmodelle übergeben.

        Da der Begriff „Vektorstore“ in der Literatur häufig als Sammelbezeichnung für leichtgewichtige, embeddingbasierte Speichersysteme verwendet wird,
        ist eine Abgrenzung zu vollwertigen Vektordatenbanken notwendig. 
        Letztere integrieren zusätzliche Mechanismen wie Sharding, interne Partitionierung, Caching oder Replikation 
        und unterscheiden sich damit deutlich in ihrer Komplexität und Skalierbarkeit.

        Im Folgenden werden zunächst die grundlegenden Konzepte erläutert, bevor
        die für Vektordatenbanken zentralen Speicherverfahren systematisch dargestellt werden.
            

        \subsubsection{Abgrenzung Vektor-Store und Vektor-Datenbank}
            
            Der Begriff \emph{Vektor-Store} wird in der Literatur nicht einheitlich verwendet und dient häufig als 
            Sammelbezeichnung für leichtgewichtige Systeme, die Embeddings speichern und eine grundlegende 
            semantische Ähnlichkeitssuche bereitstellen. Solche Systeme konzentrieren sich in der Regel auf das 
            Einfügen von Embeddings und die Ausführung von k-nächste-Nachbarn-Abfragen, ohne jedoch 
            erweiterte Datenbankfunktionen bereitzustellen.

            Unter einer \emph{Vektordatenbank} werden hingegen vollwertige Datenbanksysteme verstanden, die 
            neben der Speicherung und Suche von Embeddings zusätzliche Verwaltungs- und Infrastrukturmechanismen 
            integrieren. Dazu zählen insbesondere Sharding, interne Partitionierung, Caching, Replikation, 
            Indexstrukturen für Approximate Nearest Neighbor (ANN) sowie Metadatenverwaltung und Konsistenzmodelle. 
            Diese Systeme sind auf hohe Skalierbarkeit, Robustheit und Performanz im produktiven Einsatz ausgelegt.

            Für die vorliegende Arbeit wird daher folgende Unterscheidung getroffen:

            \begin{itemize}
                \item \textbf{Vektor-Store:} leichtgewichtiges System zur Speicherung von Embeddings und zur 
                Durchführung semantischer Ähnlichkeitssuchen.
                \item \textbf{Vektordatenbank:} vollständiges Datenbankmanagementsystem mit skalierbaren 
                Speicher- und Verwaltungsmechanismen.
            \end{itemize}

            Diese definitorische Abgrenzung dient der Klarheit und legt die einheitliche Verwendung der Begriffe im weiteren Verlauf der Arbeit fest.

        \subsubsection{Speicherverfahren für Vektordatenbanken}
             \textbf{horizontale Datenpartitionierung über mehrere Maschinen} (auch Sharding genannt) bezeichnet ein Verfahren, 
             bei dem eine Datenbank in mehrere logisch getrennte und auf verschiedene physische Knoten verteilte Teilmengen („Shards“) aufgeteilt wird. 
             Durch diese Aufteilung wird das Gesamtdatenset in kleinere, handhabbarere Einheiten zerlegt, 
             was Skalierbarkeit, Lastverteilung und die Verwaltung großer Datenmengen erleichtert (vgl. \cite{ma_comprehensive_2025} S.~3).

            \textbf{horizontale Datenpartitionierung innerhalb einer Maschine} Die horizontale Datenpartitionierung innerhalb einer Maschine 
            bezeichnet die Aufteilung der in einer einzelnen Datenbankinstanz gespeicherten Vektordaten in mehrere 
            logisch getrennte Teilmengen („Partitionen“). Im Unterschied zum Sharding, das Daten über mehrere physische Knoten verteilt, 
            erfolgt diese Form der Partitionierung ausschließlich innerhalb eines Systems.
            Ziel ist es, lokale Abfragen effizienter auszuführen, Speicherressourcen besser auszunutzen und parallele Verarbeitung zu ermöglichen.
            Partitionen können beispielsweise über Wertebereiche (Range-Partitioning), vordefinierte Kategorien (List-Partitioning) oder Hash-Verfahren gebildet werden.
            Durch diese interne Strukturierung müssen Suchanfragen nur gegen relevante Partitionen ausgeführt werden, 
            was insbesondere bei großen Einbettungsräumen die Latenz der semantischen Ähnlichkeitssuche reduziert (vgl. \cite{ma_comprehensive_2025}, S.~3f.).

            In modernen Vektordatenbanken wird Partitionierung häufig mit Sharding kombiniert: 
            
            Während Sharding die Skalierung über mehrere Knoten ermöglicht, 
            optimiert die interne Partitionierung die Datenorganisation innerhalb jedes Shards. 
            Beide Verfahren bilden damit die Grundlage für performante Retrieval-Systeme in Vektor Stores und Vektordatenbanken.

        \textbf{Caching-Mechanismen in Vektordatenbanken} beschreiben das Zwischenspeichern häufig oder kürzlich genutzter Daten in besonders schnellen Speichermedien (z.B. RAM), 
            um Zugriffszeiten zu reduzieren und die Last auf der eigentlichen Datenbank zu verringern. 
            Für Vektordatenbanken ist Caching ein zentraler Mechanismus, da semantische Ähnlichkeitssuchen typischerweise 
            rechenintensiv sind und von wiederholten Abfragen profitieren (vgl. \cite{ma_comprehensive_2025}, S.~4f.).

            Im Gegensatz zu klassischen Schlüssel-Wert-Caches (etwa Redis) ist das Caching in VDBs herausfordernder, 
            da Embeddings hochdimensionale Vektoren darstellen und identische Anfragen selten auftreten. 
            Daher kommen allgemeine Cache-Strategien zum Einsatz, die unabhängig vom genauen Vektorinhalt arbeiten,
            da der Vergleich hochdimensionaler Vektoren zur Bestimmung von Cache-Schlüsseln ungeeignet ist. 
           
            Zu den gängigen Verfahren zählen:
                
            \begin{itemize}
                \item \textbf{First-In First-Out (FIFO):} entfernt das älteste Element im Cache; einfach, aber
                ohne Berücksichtigung der Zugriffshäufigkeit.
                \item \textbf{Least Recently Used (LRU):} löscht den am längsten nicht genutzten Eintrag; gut
                geeignet für Arbeitslasten mit zeitlicher Lokalität.
                \item \textbf{Most Recently Used (MRU):} entfernt das zuletzt genutzte Element; sinnvoll bei
                einmaligen Zugriffsmustern.
                \item \textbf{Least Frequently Used (LFU):} bevorzugt das Entfernen seltener genutzter Einträge;
                vorteilhaft bei stabilen Zugriffshäufigkeiten.
            \end{itemize}

                Einige Systeme nutzen zudem \textbf{partitioniertes Caching}, bei dem Vektordaten in Gruppen (z.B. nach Kategorien oder Zugriffsmustern) getrennt gecacht werden, um Ressourcen gezielt zu optimieren. 
                Insgesamt trägt Caching wesentlich zur Reduktion der Abfragelatenz und zur Stabilisierung der Systemlast bei, insbesondere bei wiederkehrenden Ähnlichkeitsanfragen.


        \textbf{Replikation} bezeichnet das Anlegen und Verteilen mehrerer Kopien von Vektordaten
            auf unterschiedliche Knoten eines verteilten Systems, um Ausfallsicherheit, Verfügbarkeit und Leselastverteilung zu erhöhen.
            Während sie für die semantische Ähnlichkeitssuche nicht unmittelbar leistungsbestimmend ist, stellt sie einen zentralen Mechanismus für die
            betriebliche Robustheit moderner Vektordatenbanken dar. 
            In der Literatur werden drei grundlegende Replikationsstrategien unterschieden (vgl.~\cite{ma_comprehensive_2025}, S.~5f.).

            Für den im Rahmen dieser Arbeit entwickelten Prototypen spielt Replikation 
            keine operative Rolle, da der Vektorstore in einer Single-Node-Architektur betrieben wird. 
            Der folgende Abschnitt dient daher der theoretischen Einordnung, um die Funktionsweise produktiver Vektordatenbanksysteme besser zu verstehen.

            \textbf{Leader-Follower-Replikation}  
            Bei der Leader-Follower-Replikation übernimmt ein einzelner Knoten die Rolle
            des Leaders und verarbeitet sämtliche Schreiboperationen. Die erzeugten Updates
            werden anschließend an mehrere Follower-Knoten repliziert, die ausschließlich
            Leseanfragen bearbeiten (vgl.~\cite{ma_comprehensive_2025}, S.~5f.). 
            
            Dieses Modell ermöglicht eine klare Konsistenzsemantik, da alle Änderungen zentral koordiniert werden. 
            Gleichzeitig entsteht jedoch ein Single Point of Failure, was eine zusätzliche Failover-Mechanik erforderlich macht.

            \textbf{Multi-Leader-Replikation}  
            Mehrere Knoten können parallel Schreiboperationen entgegennehmen. 
            Die dabei entstehenden Aktualisierungen werden zwischen den beteiligten Knoten ausgetauscht. 
            Dieses Modell erhöht die Schreibkapazität, erfordert jedoch Strategien zur Auflösung konkurrierender Updates (vgl.~\cite{ma_comprehensive_2025}, S.~5f.).

            \textbf{Leaderless-Replikation}  
            Leaderless-Replikation verzichtet vollständig auf die Unterscheidung zwischen Leader- und Follower-Knoten. 
            Jeder Knoten kann Lese- und Schreiboperationen ausführen, wodurch weder zentrale Engpässe noch einzelne Ausfallpunkte entstehen. 
            Konsistenz wird über Quorum-Modelle oder koordinationsbasierte Protokolle sichergestellt. 
            Dieses Modell bietet hohe Fehlertoleranz, bringt aber ggf. erhöhte Komplexität hinsichtlich Konsistenz- und Konfliktbehandlung mit sich (vgl.~\cite{ma_comprehensive_2025}, S.~5f.).

            Insgesamt trägt Replikation wesentlich zur Robustheit und Verfügbarkeit verteilter Vektordatenbanksysteme bei, steht jedoch weniger im direkten
            Zentrum der Ähnlichkeitssuche als vielmehr ihrer infrastrukturellen Betriebsstabilität.

            Da die vorliegende Arbeit den Fokus auf die Funktionsweise von Vektor-Stores im Rahmen einer RAG-Architektur legt und nicht auf die Untersuchung verteilter 
            Systemszenarien abzielt, wird Replikation in dieser Arbeit ausschließlich theoretisch betrachtet. 
            Die beschriebenen Replikationsmechanismen sind jedoch wesentliche Bestandteile produktiver Vektordatenbanksysteme und dienen der 
            Verortung der später eingesetzten Technologie im Gesamtkontext verteilter Datenbanksysteme.

        \subsubsection{Suchverfahren}
        \subsubsection{Integration in LLMs (RAG)}
            

            
    \subsection{Neuronale Netze}
    \subsection{Large Language Models}
    \subsection{Retrieval-Augmented Generation} % LLM mit Datenquelle
    \subsection{Schnittstellentechnologie}
    \subsection{prompt Engineering}


\section{Anforderungsanalyse}
    \subsection{Zielgruppenanalyse}
    \subsection{Benötigte Daten aus dem PIM System}
    \subsection{Vergleich der LLM Modelle}


\section{Konzeption und Realisierung}
    \subsection{Architektur}
    \subsection{Datenfluss zwischen Vektor Store und Applikation}
    \subsection{Schnittstellendesign}
    \subsection{Promptdesign}


\section{Implementierung des Prototyps}
    \subsection{Überblick über die Systemkomponenten}
    \subsection{Umsetzung der Schnittstellen (Vector Store / Applikation / LLM)}
    \subsection{Datenimport und -export}
    \subsection{Integration des LLMs und Promptlogik}
    \subsection{Fehlerbehandlung und Parallelität}


\section{Evaluation}
        \subsection{Aufbau der Evaluationsbewertung}
        \subsection{Bewertungsmetrik/ -kriterien} %
        \subsection{Durchführung}
        Wer bewertet Texte, Bewertungsbogen, Wie viele Bewertungen pro Text 

        \subsection{Ergebnis}
        
        \subsection{Diskussion}
            LLM ohne RAG halluziniert → schlecht für PIM-Daten (Faktentreue).

            Prompt mit Rollenbeschreibung („Du bist ein Marketing-Experte…“) besser als generischer Prompt.

            Techniktexte brauchen präzisere Struktur → Template hilft.

            Marketingtexte profitieren von höherer Kreativität → Temperatur-Einstellungen diskutieren.
            
        

\printbibliography  % Literaturverzeichnis einfügen



\end{document}


%Quellen