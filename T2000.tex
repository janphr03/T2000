\documentclass[12pt,a4paper]{article} % Standard für Hausarbeiten

% Sprachunterst\"utzung f\"ur deutsche Umlaute und Silbentrennung
\usepackage[utf8]{inputenc}  % Zeichencodierung
\usepackage[T1]{fontenc}     % Korrekte Darstellung von Umlauten
\usepackage[ngerman]{babel}  % Deutsche Sprache und Silbentrennung
\usepackage{csquotes}

% Seitenr\"ander sch\"oner machen
\usepackage[a4paper, left=3cm, right=3cm, top=2.5cm, bottom=2.5cm]{geometry}

% Mathematik-Symbole
\usepackage{amsmath, amssymb}

% Grafiken und Bilder einf\"ugen
\usepackage{graphicx}  
\usepackage{float}      % Bessere Kontrolle \über die Platzierung

% Tabellen verbessern
\usepackage{array, booktabs}

% Literaturverzeichnis mit BibTeX
\usepackage[style=ieee, backend=biber]{biblatex}
\addbibresource{literature.bib}
\addbibresource{Meine Bibliothek.bib}

% Quellcode schön darstellen
\usepackage{listings}
\usepackage{xcolor}

\usepackage{tikz}
\usetikzlibrary{positioning}


\usepackage{needspace} % in der Präambel

\usepackage[acronym]{glossaries}



% --- Grundbegriffe KI / ML ---
\newacronym{ki}{KI}{Künstliche Intelligenz}
\newacronym{ml}{ML}{Maschinelles Lernen}
\newacronym{llm}{LLM}{Large Language Model}
\newacronym{rag}{RAG}{Retrieval-Augmented Generation}
\newacronym{embedding}{Embedding}{Numerische Repräsentation von Text oder Objekten im Vektorraum}

% --- NNS / ANN ---
\newacronym{nns}{NNS}{Nearest Neighbor Search, Verfahren zur Suche nach nächsten Nachbarn}
\newacronym{ann}{ANN}{Approximate Nearest Neighbor, Näherungsverfahren für schnelle Ähnlichkeitssuche}
\newacronym{knn}{kNN}{k-Nächste-Nachbarn-Suche}

% --- Vektorstore / Vektordatenbank ---
\newacronym{vektorstore}{Vektor-Store}{Leichtgewichtiges System zur Speicherung von Embeddings und Ausführung semantischer Ähnlichkeitssuchen}
\newacronym{vektordatenbank}{Vektordatenbank}{Datenbanksystem für Embeddings mit Mechanismen wie Sharding, Partitionierung, Caching und Replikation}

% --- Speicherverfahren ---
\newacronym{sharding}{Sharding}{Horizontale Datenpartitionierung über mehrere Maschinen zur Skalierung und Lastverteilung}
\newacronym{partitionierung}{Partitionierung}{Aufteilung eines Datensatzes in logisch getrennte Teilmengen}
\newacronym{rangepartition}{Range-Partitioning}{Partitionierung nach Wertebereichen}
\newacronym{listpartition}{List-Partitioning}{Partitionierung anhand expliziter Wertelisten}
\newacronym{hashpartition}{Hash-Partitioning}{Partitionierung mithilfe einer Hashfunktion}

% --- Caching ---
\newacronym{caching}{Caching}{Zwischenspeicherung häufig genutzter Daten zur Reduktion von Latenz und Systemlast}
\newacronym{fifo}{FIFO}{Cache-Strategie: First-In First-Out}
\newacronym{lru}{LRU}{Cache-Strategie: Least Recently Used}
\newacronym{mru}{MRU}{Cache-Strategie: Most Recently Used}
\newacronym{lfu}{LFU}{Cache-Strategie: Least Frequently Used}

% --- Replikation ---
\newacronym{replikation}{Replikation}{Anlegen mehrerer Datenkopien zur Erhöhung der Verfügbarkeit und Ausfallsicherheit}
\newacronym{leaderfollower}{Leader-Follower-Replikation}{Replikationsmodell mit einem schreibenden Leader und mehreren lesenden Followern}
\newacronym{multileader}{Multi-Leader-Replikation}{Replikationsmodell mit mehreren parallel schreibenden Knoten}
\newacronym{leaderless}{Leaderless-Replikation}{Replikationsmodell ohne Leader; Konsistenz über Quorum-Protokolle}

% --- Tree-Based Search ---
\newacronym{kd-tree}{KD-Tree}{Baumstruktur zur Organisation von Datenpunkten in k-dimensionalen Räumen}
\newacronym{ball-tree}{Ball-Tree}{Baumstruktur, die Daten in hypersphärischen Regionen gruppiert}
\newacronym{r-tree}{R-Tree}{Baumstruktur zur Organisation von Daten in hyperrechteckigen Bereichen}
\newacronym{m-tree}{M-Tree}{Baumstruktur auf Basis von Distanzen und Covering Radii}

% --- Geometrische Begriffe ---
\newacronym{mbr}{MBR}{Minimum Bounding Rectangle, kleinstes umschließendes Rechteck einer Punktmenge}
\newacronym{coveringradius}{Covering Radius}{Maximale Distanz zwischen einem Routing-Objekt und seinen Kindobjekten}


\makeglossaries



\lstset{ 
    language=Python, % oder C++, Java, Python etc.
    basicstyle=\ttfamily\small, 
    keywordstyle=\color{blue}, 
    commentstyle=\color{gray}, 
    stringstyle=\color{red},
    breaklines=true
}

%Commands/ Style-Definitions====================================
\newcommand{\csharp}{C\#}

\setlength{\parskip}{1em}    % Abstand zwischen Abs\"atzen
\setlength{\parindent}{0pt}  % Kein Einzug bei neuen Abs\"atzen


% compile \"uber Konsole
% pdflatex test.tex
% biber test
% dann wieder pdflatex test.tex
% ba Datei Pfad anpassen
%T999-Beginn==================================================
\begin{document}


\title{T2000} 
\author{Jan Herrmann}
\date{\today}
\maketitle

%\section{Test}
%\Das ist meine LaTeX-Hausarbeit.
%\parencite{heimeshoff_certification_2024}
%testZitat
%\parencite{kim_planck_2024}


\tableofcontents
\newpage

%–– Im Dokument dann an der Stelle, an der du dein Abkürzungsverzeichnis haben willst ––
\printglossary[type=\acronymtype,title=Abkürzungsverzeichnis]



\section{Einleitung}
    \subsection{Motivation}
    \subsection{Problemstellung}
    \subsection{Zielsetzung}

DEVELOP Test
\acrshort{ML}


\section{Theoretischer Hintergrund}
    


        \subsection{Künstliche Intelligenz}

            Bevor eine präzise Definition von künstlicher Intelligenz möglich ist, muss geklärt werden, welches Ziel ein intelligentes System verfolgen soll. 
            Russell und Norvig zeigen, dass gängige KI-Definitionen in der wissenschaftlichen Literatur entlang zweier zentraler Dimensionen variieren (vgl. \cite{russell_artificial_1995}, Kap.~1.1):

            \begin{itemize}
                \item \textbf{Mensch vs. Rationalität} Soll ein System wie ein Mensch denken oder handeln, oder soll es unabhängig vom Menschen ideal rational agieren?
                \item \textbf{Denken vs. Handeln} Soll Intelligenz anhand interner Denkprozesse oder anhand des beobachtbaren Verhaltens beurteilt werden?
            \end{itemize}

            Aus diesen beiden Dimensionen ergeben sich vier grundlegende Perspektiven auf KI, die unterschiedliche historische Forschungsrichtungen geprägt haben. 
            Eine Übersicht dieser Einordnung zeigt Tabelle~\ref{tab:ki-perspektiven}
        

            \begin{table}[h]
            \centering
            \begin{tabular}{|p{4cm}|p{9cm}|}
            \hline
            \textbf{Kategorie} & \textbf{Beschreibung} \\ \hline
            Systeme, die wie Menschen denken & Fokus auf Nachbildung menschlicher Denkprozesse, z.B. durch kognitive Modelle oder psychologische Theorien. \\ \hline
            Systeme, die wie Menschen handeln & Intelligenz wird anhand menschlich ähnlichen Verhaltens beurteilt, unabhängig vom zugrunde liegenden Denkprozess. \\ \hline
            Systeme, die rational denken & Fokus auf logische Schlussfolgerungen und formale Wissensrepräsentation. \\ \hline
            Systeme, die rational handeln & Intelligente Agenten handeln zielgerichtet und optimal in ihrer Umgebung. \\ \hline
            \end{tabular}
            \caption{Eigene Darstellung in Anlehnung an \cite{russell_artificial_1995}, Kap.~1.1}
            \label{tab:ki-perspektiven}
            \end{table}


            \textbf{Abgrenzung von KI und ML}

            Künstliche Intelligenz (\acrshort{ki}) umfasst alle Verfahren, die darauf abzielen,
            intelligentes Verhalten technisch zu realisieren. Dazu gehören sowohl symbolische
            Ansätze wie Wissensrepräsentation und logisches Schließen als auch
            datengetriebene Methoden zur Wahrnehmung oder Sprachverarbeitung
            (vgl. \cite{russell_artificial_1995}, Kap.~1.1).

            Maschinelles Lernen (\acrshort{ml}) stellt ein klar abgegrenztes Teilgebiet der KI dar.
            Russell und Norvig beschreiben es als das Teilfeld der künstlichen Intelligenz,
            das sich mit Programmen befasst, die aus Erfahrung lernen
            (vgl. \cite{russell_artificial_1995}, Einleitung zu Teil~VI).
            

            Während KI somit als Oberbegriff sämtliche Methoden intelligenter Problemlösung
            einschließt, konzentriert sich ML ausschließlich auf Verfahren, die Wissen nicht
            explizit vorgegeben bekommen, sondern selbstständig aus Daten oder Erfahrungen
            erschließen. ML bildet damit die Grundlage für viele moderne KI-Anwendungen,
            insbesondere für datengetriebene Systeme wie neuronale Netze oder Large Language Models.
            
            
        \begin{figure}[htbp]
            \centering
                \begin{tikzpicture}[font=\small]

                    % KI (Obermenge)
                    \filldraw[fill=gray!9, draw=gray!70, thick] (0,0) circle (3cm);
                    \node[font=\normalsize\bfseries] at (-0.4 , 3.2) {Künstliche Intelligenz};

                    % ML (Teilmenge in KI)
                    \filldraw[fill=gray!24, draw=gray!80, thick] (0.9,-0.2) circle (2cm);
                    \node[font=\normalsize\bfseries] at (0.8 , 0.0) {Maschinelles Lernen};

                \end{tikzpicture}
            \caption{Einordnung von Maschinellem Lernen als Teilgebiete der Künstlichen Intelligenz}
            \label{fig:ki-ml-dl}
        \end{figure} 


        

    \subsection{Vektor-Store}
        
        \acrshort{vektorstore}s bilden die Grundlage für moderne KI-Anwendungen, die auf semantischer Ähnlichkeitssuche basieren. Sie dienen der Speicherung von
        Embeddings, also numerischen Repräsentationen von Text- oder Produktdaten, und ermöglichen effiziente Nearest-Neighbor-Abfragen. 
        Damit stellen sie einen zentralen Baustein für Retrieval-Augmented Generation (RAG) und für Systeme dar, die kontextbezogene Informationen an große Sprachmodelle übergeben.

        Da der Begriff „Vektorstore“ in der Literatur häufig als Sammelbezeichnung für leichtgewichtige, embeddingbasierte Speichersysteme verwendet wird,
        ist eine Abgrenzung zu vollwertigen Vektordatenbanken notwendig. 
        Letztere integrieren zusätzliche Mechanismen wie Sharding, interne Partitionierung, Caching oder Replikation 
        und unterscheiden sich damit deutlich in ihrer Komplexität und Skalierbarkeit.

        Im Folgenden werden zunächst die grundlegenden Konzepte erläutert, bevor
        die für Vektordatenbanken zentralen Speicherverfahren systematisch dargestellt werden.
            

        \subsubsection{Abgrenzung Vektor-Store und Vektor-Datenbank}
            
            Der Begriff \acrshort{vektorstore} wird in der Literatur nicht einheitlich verwendet und dient häufig als 
            Sammelbezeichnung für leichtgewichtige Systeme, die Embeddings speichern und eine grundlegende 
            semantische Ähnlichkeitssuche bereitstellen. Solche Systeme konzentrieren sich in der Regel auf das 
            Einfügen von Embeddings und die Ausführung von k-nächste-Nachbarn-Abfragen, ohne jedoch 
            erweiterte Datenbankfunktionen bereitzustellen.

            Unter einer \acrshort{vektordatenbank} werden hingegen vollwertige Datenbanksysteme verstanden, die 
            neben der Speicherung und Suche von Embeddings zusätzliche Verwaltungs- und Infrastrukturmechanismen 
            integrieren. Dazu zählen insbesondere Sharding, interne Partitionierung, Caching, Replikation, 
            Indexstrukturen für Approximate Nearest Neighbor (ANN) sowie Metadatenverwaltung und Konsistenzmodelle. 
            Diese Systeme sind auf hohe Skalierbarkeit, Robustheit und Performanz im produktiven Einsatz ausgelegt.

            Für die vorliegende Arbeit wird daher folgende Unterscheidung getroffen:

            \begin{itemize}
                \item \textbf{Vektor-Store:} leichtgewichtiges System zur Speicherung von Embeddings und zur 
                Durchführung semantischer Ähnlichkeitssuchen.
                \item \textbf{Vektordatenbank:} vollständiges Datenbankmanagementsystem mit skalierbaren 
                Speicher- und Verwaltungsmechanismen.
            \end{itemize}

            Diese definitorische Abgrenzung dient der Klarheit und legt die einheitliche Verwendung der Begriffe im weiteren Verlauf der Arbeit fest.

        \subsubsection{Speicherverfahren für Vektordatenbanken}
            Die im Folgenden beschriebenen Mechanismen stellen zentrale Bestandteile moderner, skalierbarer Vektordatenbanksysteme dar. Sie werden im produktiven Umfeld zur 
            Optimierung von Leistung, Verfügbarkeit und Robustheit eingesetzt. 
            Für den im Rahmen dieser Arbeit entwickelten Prototypen spielen diese Konzepte jedoch keine operative Rolle, 
            da ein leichtgewichtiges Single-Node-System ohne verteilte Architektur eingesetzt wird. 
            Die Ausführungen dienen daher der theoretischen Einordnung und sollen den technologischen Kontext verdeutlichen, in dem sich Vektordatenbanken typischerweise bewegen.


             \textbf{horizontale Datenpartitionierung über mehrere Maschinen} (auch Sharding genannt) bezeichnet ein Verfahren, 
             bei dem eine Datenbank in mehrere logisch getrennte und auf verschiedene physische Knoten verteilte Teilmengen („Shards“) aufgeteilt wird. 
             Durch diese Aufteilung wird das Gesamtdatenset in kleinere, handhabbarere Einheiten zerlegt, 
             was Skalierbarkeit, Lastverteilung und die Verwaltung großer Datenmengen erleichtert (vgl. \cite{ma_comprehensive_2025} S.~3).

            \textbf{horizontale Datenpartitionierung innerhalb einer Maschine} Die horizontale Datenpartitionierung innerhalb einer Maschine 
            bezeichnet die Aufteilung der in einer einzelnen Datenbankinstanz gespeicherten Vektordaten in mehrere 
            logisch getrennte Teilmengen („Partitionen“). Im Unterschied zum Sharding, das Daten über mehrere physische Knoten verteilt, 
            erfolgt diese Form der Partitionierung ausschließlich innerhalb eines Systems.
            Ziel ist es, lokale Abfragen effizienter auszuführen, Speicherressourcen besser auszunutzen und parallele Verarbeitung zu ermöglichen.
            Partitionen können beispielsweise über Wertebereiche (Range-Partitioning), vordefinierte Kategorien (List-Partitioning) oder Hash-Verfahren gebildet werden.
            Durch diese interne Strukturierung müssen Suchanfragen nur gegen relevante Partitionen ausgeführt werden, 
            was insbesondere bei großen Einbettungsräumen die Latenz der semantischen Ähnlichkeitssuche reduziert (vgl. \cite{ma_comprehensive_2025}, S.~3f.).

            In modernen Vektordatenbanken wird Partitionierung häufig mit Sharding kombiniert: 
            
            Während Sharding die Skalierung über mehrere Knoten ermöglicht, 
            optimiert die interne Partitionierung die Datenorganisation innerhalb jedes Shards. 
            Beide Verfahren bilden damit die Grundlage für performante Retrieval-Systeme in Vektor Stores und Vektordatenbanken.

            
        \textbf{Caching-Mechanismen in Vektordatenbanken} beschreiben das Zwischenspeichern häufig oder kürzlich genutzter Daten in besonders schnellen Speichermedien (z.B. RAM), 
            um Zugriffszeiten zu reduzieren und die Last auf der eigentlichen Datenbank zu verringern. 
            Für Vektordatenbanken ist Caching ein zentraler Mechanismus, da semantische Ähnlichkeitssuchen typischerweise 
            rechenintensiv sind und von wiederholten Abfragen profitieren (vgl. \cite{ma_comprehensive_2025}, S.~4f.).

            Im Gegensatz zu klassischen Schlüssel-Wert-Caches (etwa Redis) ist das Caching in VDBs herausfordernder, 
            da Embeddings hochdimensionale Vektoren darstellen und identische Anfragen selten auftreten. 
            Daher kommen allgemeine Cache-Strategien zum Einsatz, die unabhängig vom genauen Vektorinhalt arbeiten,
            da der Vergleich hochdimensionaler Vektoren zur Bestimmung von Cache-Schlüsseln ungeeignet ist. 
           
            Zu den gängigen Verfahren zählen:
                
            \begin{itemize}
                \item \textbf{First-In First-Out (FIFO):} entfernt das älteste Element im Cache; einfach, aber
                ohne Berücksichtigung der Zugriffshäufigkeit.
                \item \textbf{Least Recently Used (LRU):} löscht den am längsten nicht genutzten Eintrag; gut
                geeignet für Arbeitslasten mit zeitlicher Lokalität.
                \item \textbf{Most Recently Used (MRU):} entfernt das zuletzt genutzte Element; sinnvoll bei
                einmaligen Zugriffsmustern.
                \item \textbf{Least Frequently Used (LFU):} bevorzugt das Entfernen seltener genutzter Einträge;
                vorteilhaft bei stabilen Zugriffshäufigkeiten.
            \end{itemize}

                Einige Systeme nutzen zudem \textbf{partitioniertes Caching}, bei dem Vektordaten in Gruppen (z.B. nach Kategorien oder Zugriffsmustern) getrennt gecacht werden, um Ressourcen gezielt zu optimieren. 
                Insgesamt trägt Caching wesentlich zur Reduktion der Abfragelatenz und zur Stabilisierung der Systemlast bei, insbesondere bei wiederkehrenden Ähnlichkeitsanfragen.


        \textbf{Replikation} bezeichnet das Anlegen und Verteilen mehrerer Kopien von Vektordaten
            auf unterschiedliche Knoten eines verteilten Systems, um Ausfallsicherheit, Verfügbarkeit und Leselastverteilung zu erhöhen.
            Während sie für die semantische Ähnlichkeitssuche nicht unmittelbar leistungsbestimmend ist, stellt sie einen zentralen Mechanismus für die
            betriebliche Robustheit moderner Vektordatenbanken dar. 
            In der Literatur werden drei grundlegende Replikationsstrategien unterschieden (vgl.~\cite{ma_comprehensive_2025}, S.~5f.).

            \textbf{Leader-Follower-Replikation}  
            Bei der Leader-Follower-Replikation übernimmt ein einzelner Knoten die Rolle des Leaders und verarbeitet sämtliche Schreiboperationen. 
            Die erzeugten Updates werden anschließend an mehrere Follower-Knoten repliziert, die ausschließlich Leseanfragen bearbeiten (vgl.~\cite{ma_comprehensive_2025}, S.~5f.). 
            
            Dieses Modell ermöglicht eine klare Konsistenzsemantik, da alle Änderungen zentral koordiniert werden. 
            Gleichzeitig entsteht jedoch ein Single Point of Failure, was eine zusätzliche Failover-Mechanik erforderlich macht.

            \textbf{Multi-Leader-Replikation}  
            Mehrere Knoten können parallel Schreiboperationen entgegennehmen. 
            Die dabei entstehenden Aktualisierungen werden zwischen den beteiligten Knoten ausgetauscht. 
            Dieses Modell erhöht die Schreibkapazität, erfordert jedoch Strategien zur Auflösung konkurrierender Updates (vgl.~\cite{ma_comprehensive_2025}, S.~5f.).

            \textbf{Leaderless-Replikation}  
            Leaderless-Replikation verzichtet vollständig auf die Unterscheidung zwischen Leader- und Follower-Knoten. 
            Jeder Knoten kann Lese- und Schreiboperationen ausführen, wodurch weder zentrale Engpässe noch einzelne Ausfallpunkte entstehen. 
            Konsistenz wird über Quorum-Modelle oder koordinationsbasierte Protokolle sichergestellt. 
            Dieses Modell bietet hohe Fehlertoleranz, bringt aber ggf. erhöhte Komplexität hinsichtlich Konsistenz- und Konfliktbehandlung mit sich (vgl.~\cite{ma_comprehensive_2025}, S.~5f.).

            Insgesamt trägt Replikation wesentlich zur Robustheit und Verfügbarkeit verteilter Vektordatenbanksysteme bei, steht jedoch weniger im direkten
            Zentrum der Ähnlichkeitssuche als vielmehr ihrer infrastrukturellen Betriebsstabilität.

        \subsubsection{Suchverfahren}
            \textbf{}

        \subsubsection{Integration in LLMs (RAG)}
            

            
    \subsection{Neuronale Netze}
    \subsection{Large Language Models}
    \subsection{Retrieval-Augmented Generation} % LLM mit Datenquelle
    \subsection{Schnittstellentechnologie}
    \subsection{prompt Engineering}


\section{Anforderungsanalyse}
    \subsection{Analyse}
    \subsection{Benötigte Daten aus dem PIM System}
    \subsection{Vergleich der LLM Modelle}


\section{Konzeption}
    \subsection{Architektur}
    \subsection{Datenfluss zwischen Vektor Store und Applikation}
    \subsection{Schnittstellendesign}
    \subsection{Promptdesign}


\section{Implementierung}
    \subsection{Überblick über die Systemkomponenten}
    \subsection{Umsetzung der Schnittstellen (Vector Store / Applikation / LLM)}
    \subsection{Datenimport und -export}
    \subsection{Integration des LLMs und Promptlogik}
    \subsection{Fehlerbehandlung und Parallelität}


\section{Evaluation}
        \subsection{Aufbau der Evaluationsbewertung}
        \subsection{Bewertungsmetrik/ -kriterien} %
        \subsection{Durchführung}
        Wer bewertet Texte, Bewertungsbogen, Wie viele Bewertungen pro Text 

        \subsection{Ergebnis}
        
        \subsection{Diskussion}
            LLM ohne RAG halluziniert → schlecht für PIM-Daten (Faktentreue).

            Prompt mit Rollenbeschreibung („Du bist ein Marketing-Experte…“) besser als generischer Prompt.

            Techniktexte brauchen präzisere Struktur -> Template hilft.

            Marketingtexte profitieren von höherer Kreativität → Temperatur-Einstellungen diskutieren.
            
        

\printbibliography  % Literaturverzeichnis einfügen



\end{document}


%Quellen